{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDNAx7b4guAU"
   },
   "source": [
    "# Work Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5pe51BFfyWO"
   },
   "source": [
    "\n",
    "# Observations from our documents:\n",
    "- They contain chemical signals such as images, tables, structured text, and varied formats (bold, italics, headers),\n",
    "- I do not want to include headers or footers (noise),\n",
    "- They are in English\n",
    "\n",
    "## We work with multi-structure PDFs. A multi-structure PDF may include:\n",
    "\n",
    "* Continuous text (paragraphs).\n",
    "* Tables with organized data.\n",
    "* Images with embedded text (requires OCR).\n",
    "* Headers, titles, sections.\n",
    "* Margins, notes, columns, footers.\n",
    "\n",
    "These elements can vary greatly between documents, so we need a modular and flexible approach.\n",
    "\n",
    "# Objective:\n",
    "Extract the useful and structured content from each PDF, ignoring noise (headers, footers), preserving tables, and respecting the textual hierarchy (titles, sections, etc.).\n",
    "\n",
    "# Strategy\n",
    "\n",
    "1. Convert PDFs to Markdown using OpenAI\n",
    "2. Standardize to Document format to be used with LangChain\n",
    "3. Split into chunks\n",
    "4. Embedding\n",
    "5. Create a vector database with Chroma\n",
    "\n",
    "# Considerations\n",
    "- Extraction of structured metadata for sources, citations, and authorities\n",
    "- GraphRAG\n",
    "- Images\n",
    "- Chunking by tokens or structure\n",
    "- Which embedding to use (OpenAI or others)\n",
    "\n",
    "# Justify decisions\n",
    "- LangChain is compatible with all major embedding model providers such as OpenAI, Cohere, HuggingFace, etc. They are implemented as Embedding classes and provide two methods: one for embedding documents and another for embedding queries (requests).\n",
    "- GPT-4o mini is a smaller and more affordable version of OpenAI's GPT-4o model, offering a balance between performance and cost-effectiveness for various AI applications.\n",
    "- Chroma is an open-source embedding database designed to facilitate the development of applications using language models (LLMs). It allows storing texts, converting them into vectors, and performing similarity searches efficiently. It integrates easily with tools like LangChain (in Python and JavaScript) and LlamaIndex.\n",
    "\n",
    "# Embedding Selection\n",
    "\n",
    "## Prediction-based:\n",
    "\n",
    "### OpenAI Embeddings:\n",
    "- text-embedding-3-small\n",
    "- text-embedding-3-large\n",
    "\n",
    "### HuggingFace / SentenceTransformers\n",
    "- all-MiniLM-L6-v2\n",
    "- all-mpnet-base-v2\n",
    "- bge-base-en\n",
    "- intfloat/e5-large-v2\n",
    "- e5\n",
    "- bge\n",
    "- GTE\n",
    "\n",
    "## Frequency-based\n",
    "- Count Vector / Tf-idf Vector\n",
    "- Co-occurrence matrices\n",
    "\n",
    "## Ad-hoc embeddings\n",
    "Embeddings specifically trained for a particular domain or task.\n",
    "\n",
    "## Evaluate\n",
    "- Recall / MRR: How well it retrieves relevant documents\n",
    "- Embedding size: Affects performance on queries\n",
    "- Inference time: Important if processing many documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBXF6xX8hQu7"
   },
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XB2w6BmbhQDV"
   },
   "outputs": [],
   "source": [
    "!pip install openai python-docx pdfplumber\n",
    "!pip install pymupdf\n",
    "!pip install -U langchain-community\n",
    "!pip install chromadb\n",
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK4b4pbwhfmH"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcFttya-hjkk"
   },
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import docx\n",
    "import fitz\n",
    "import pdfplumber\n",
    "from google.colab import drive\n",
    "import openai\n",
    "import tiktoken\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXkv3HIgtzIY"
   },
   "outputs": [],
   "source": [
    "# Configurar API KEY\n",
    "API_KEY = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rNge4wKf_G3"
   },
   "source": [
    "# Load multiple PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxWGDeSjhdHq"
   },
   "source": [
    "## Load documents from a folder in Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Gcfbil2hm_d"
   },
   "outputs": [],
   "source": [
    "# Montar Google Drive en Colab\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Q7-iVzRis9f"
   },
   "outputs": [],
   "source": [
    "# Rutas de almacenamiento\n",
    "pdf_folder = \"/content/drive/MyDrive/TFM/Grupo_1_RAG_Chemical_Safety/Notebooks/Safety Data Sheets/\"\n",
    "output_folder = \"/content/drive/MyDrive/TFM/Grupo_1_RAG_Chemical_Safety/Notebooks/output_md_openai/\"\n",
    "output_DB_Chroma = \"/content/drive/MyDrive/TFM/Grupo_1_RAG_Chemical_Safety/Notebooks/Chroma_DB/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1sg21aimAdv"
   },
   "source": [
    "# Clean documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Ja61uUqQqml"
   },
   "outputs": [],
   "source": [
    "def extract_pdf_text(pdf_path):\n",
    "    # Opens a PDF and extracts the text from all its pages\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KIZ2G3ugGS8"
   },
   "source": [
    "# Process all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZ44g3M2mMpJ"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def convert_pdfs_to_markdown(pdf_folder, output_folder, pages_per_block=5):\n",
    "    # Converts all PDFs in a folder into structured Markdown files using LLM\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        output_path = os.path.join(output_folder, os.path.splitext(filename)[0] + \".md\")\n",
    "\n",
    "        print(f\"ðŸ“„ Processing: {filename}\")\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            markdown_blocks = []\n",
    "\n",
    "            total_pages = len(doc)\n",
    "            for start_page in range(0, total_pages, pages_per_block):\n",
    "                end_page = min(start_page + pages_per_block, total_pages)\n",
    "\n",
    "                # Extract concatenated text from the pages in the block\n",
    "                block_text = \"\"\n",
    "                for p in range(start_page, end_page):\n",
    "                    block_text += doc[p].get_text() + \"\\n\\n\"\n",
    "\n",
    "                # Skip empty blocks\n",
    "                if not block_text.strip():\n",
    "                    print(f\"Block pages {start_page+1} to {end_page} empty, skipped.\")\n",
    "                    continue\n",
    "\n",
    "                prompt = f\"\"\"\n",
    "                You are an expert in data science and document conversion.\n",
    "                Convert the following text extracted from pages {start_page+1} to {end_page} of a PDF document into clean,\n",
    "                well-structured Markdown suitable for ingestion into a Retrieval-Augmented Generation (RAG) system.\n",
    "                It is essential to preserve **all relevant information** without omitting any section of the pages.\n",
    "\n",
    "                Requirements:\n",
    "                - Maintain the original hierarchical structure of headings using Markdown syntax (#, ##, ###, etc.) as accurately as possible.\n",
    "                - Correctly format lists, tables, and any logical content structures.\n",
    "                - Do NOT include Markdown code blocks (```markdown``` or any other code syntax).\n",
    "                - Do NOT add or retain titles like # Safety Data Sheet..\n",
    "                - Ensure **no content is omitted**, especially near the beginning of the pages.\n",
    "                - Keep the output in **clean, clear, and readable Markdown format**.\n",
    "                - Write all output in **English**.\n",
    "                - Ignore text styling such as underlines or colored fonts; treat all text as plain.\n",
    "                - For any images referenced on the pages, insert the image filename as a placeholder in the Markdown at the appropriate location.\n",
    "                - Remove any page numbers, headers, or footers such as \"Page 1\", \"Page 2\", etc.\n",
    "\n",
    "                Original text from pages {start_page+1} to {end_page}:{block_text}\n",
    "                \"\"\"\n",
    "\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are an expert in data science and precise document conversion to Markdown for RAG systems. Preserve semantic integrity and remove irrelevant noise.\"\n",
    "                        },\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    temperature=0.0,\n",
    "                )\n",
    "\n",
    "                markdown_block = response.choices[0].message.content.strip()\n",
    "                markdown_blocks.append(markdown_block)\n",
    "                print(f\" Block pages {start_page+1} to {end_page} processed.\")\n",
    "\n",
    "            if markdown_blocks:\n",
    "                full_markdown = \"\\n\\n---\\n\\n\".join(markdown_blocks)\n",
    "            else:\n",
    "                full_markdown = \"_Document empty or no relevant text found._\"\n",
    "\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_markdown)\n",
    "            print(f\"Full file saved at: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sA0Jpp1Jn2w"
   },
   "outputs": [],
   "source": [
    "convert_pdfs_to_markdown(pdf_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lh4ELzujglIw"
   },
   "source": [
    "# Standardize output as LangChain Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmnOZ1i6ESVU"
   },
   "outputs": [],
   "source": [
    "def load_markdown_as_documents(markdown_folder):\n",
    "    # Loads all Markdown files from a folder and returns them as LangChain Documents\n",
    "    documents = []\n",
    "    for filename in os.listdir(markdown_folder):\n",
    "        if filename.endswith(\".md\"):\n",
    "            filepath = os.path.join(markdown_folder, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                documents.append(Document(\n",
    "                    page_content=text,\n",
    "                    metadata={\"source\": filename}\n",
    "                ))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RikTo-6oV8Ji"
   },
   "outputs": [],
   "source": [
    "docs = load_markdown_as_documents(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUOoQ9RpnjgS"
   },
   "source": [
    "# Split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8R4wZsosAwD"
   },
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qy_VuA2Ht_HU"
   },
   "outputs": [],
   "source": [
    "# Chunks by Markdown structure\n",
    "def split_into_md_chunks(docs):\n",
    "    # Splits documents into chunks based on Markdown headers\n",
    "    docs_chunks_md = []\n",
    "\n",
    "    # Set splitting criteria\n",
    "    headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
    "    splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        return_each_line=False,\n",
    "        strip_headers=False\n",
    "    )\n",
    "\n",
    "    # Process documents and split into chunks\n",
    "    for doc_index, doc in enumerate(docs):\n",
    "        chunks = splitter.split_text(doc.page_content)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata.update(doc.metadata)\n",
    "            docs_chunks_md.append(chunk)\n",
    "            token_count = len(encoding.encode(chunk.page_content))\n",
    "            char_length = len(chunk.page_content)\n",
    "\n",
    "            print(f\"Doc {doc_index} - Chunk {i} - Tokens: {token_count} - Characters: {char_length}\")\n",
    "    return docs_chunks_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qmslfVIsg2n"
   },
   "outputs": [],
   "source": [
    "# Token-based chunks\n",
    "def split_into_token_chunks(docs, chunk_size=500, chunk_overlap=100):\n",
    "    # Splits documents into chunks based on token count\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    docs_chunks = []\n",
    "    for doc_index, doc in enumerate(docs):\n",
    "        chunks = splitter.split_documents([doc])\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            token_count = len(encoding.encode(chunk.page_content))\n",
    "            char_length = len(chunk.page_content)\n",
    "            print(f\"Doc {doc_index} - Chunk {i} - Tokens: {token_count} - Characters: {char_length}\")\n",
    "            docs_chunks.append(chunk)\n",
    "    return docs_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7VDlCns_cDR"
   },
   "outputs": [],
   "source": [
    "chunks = split_into_token_chunks(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cImulmT0cTGg"
   },
   "outputs": [],
   "source": [
    "# Visual inspection of chunks\n",
    "def inspect_chunks(docs_chunks_md, chunk_index):\n",
    "    # Displays information and content of a specific Markdown chunk\n",
    "    chunk = docs_chunks_md[chunk_index]\n",
    "    print(f\"Chunk {chunk_index}\")\n",
    "    print(f\"Original document: {chunk.metadata.get('source', 'unknown')}\")\n",
    "    print(f\"Tokens: {len(encoding.encode(chunk.page_content))}\")\n",
    "    print(f\"Characters: {len(chunk.page_content)}\")\n",
    "    print(f\"\\nContent:\\n{'='*80}\\n{chunk.page_content}\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Wh8H7SKn-Qh"
   },
   "outputs": [],
   "source": [
    "inspect_chunks(chunks, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "no826JxdYMdq"
   },
   "source": [
    "# Convert chunks to embeddings and create Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzW9eYkUu1sx"
   },
   "outputs": [],
   "source": [
    "# Declare embedding model compatible with GPT-4 family\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-Dp5oO7YZpg"
   },
   "outputs": [],
   "source": [
    "def create_embedding_db(docs_chunks):\n",
    "    # Build and persist a vector database from document chunks\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs_chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=output_DB_Chroma\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQJXJ3ANmJWZ"
   },
   "outputs": [],
   "source": [
    "create_embedding_db(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71xKQwKYYN5A"
   },
   "source": [
    "# RAG: Retrieve + Ask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKmkbhDfYOrK"
   },
   "outputs": [],
   "source": [
    "# query = \"3-IN-1-All-Purpose-Cleaner\"\n",
    "query = \"What are the hazard statements and recommended safety precautions for handling the 3-IN-1-All-Purpose-Cleaner, including personal protection?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3FDt6GqjDl4"
   },
   "source": [
    "## Retrieval simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mobdxhhDjCD3"
   },
   "outputs": [],
   "source": [
    "# Cargar DB Chroma\n",
    "db = Chroma(persist_directory=output_DB_Chroma, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqyn4IvX5cLI"
   },
   "outputs": [],
   "source": [
    "# Create retriever with similarity search and metadata filtering\n",
    "k = 3\n",
    "fetch_k = 20\n",
    "search_type = \"similarity\" #mmr\n",
    "# filtro_metadatos = {\"product_name\": \"3-IN-1-All-Purpose-Cleaner\"}\n",
    "retriever = db.as_retriever(\n",
    "    search_type=search_type,\n",
    "    search_kwargs={\"k\": k, \"fetch_k\": fetch_k},\n",
    "    # filter=filtro_metadatos or {}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMOYQG97pv6a"
   },
   "outputs": [],
   "source": [
    "# Retrieve relevant context\n",
    "# relevant_documents = retriever.get_relevant_documents(query)\n",
    "relevant_documents = retriever.invoke(query)\n",
    "\n",
    "# Display documents\n",
    "for i, doc in enumerate(relevant_documents):\n",
    "    print(f\"\\n - Option {i+1}\")\n",
    "    print(f\"Source document: {doc.metadata.get('source', 'unknown')}\")\n",
    "    print(f\"Detected relevant content:\\n{doc.page_content}\\n\")\n",
    "\n",
    "# Create input context\n",
    "context = \"\\n\\n---\\n\\n\".join(doc.page_content for doc in relevant_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_ehu92uxS-H"
   },
   "source": [
    "## Retrieval connected to the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syDpIp3Aw6bz"
   },
   "outputs": [],
   "source": [
    "# Estructura del prompt para el modelo\n",
    "prompt = f\"\"\"\n",
    "You are a laboratory assistant expert in safety documentation for chemical products. Based on the following extracted documentation,\n",
    "respond clearly and accurately to the user's question.Use only the information provided in the context.\n",
    "If the answer is not present, respond with: \"The information is not available in the documentation.\"\n",
    "User question:{query}\n",
    "Context: {contexto}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5at51nPbx9io"
   },
   "outputs": [],
   "source": [
    "# Send to the model\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a chemical safety assistant. Respond in English with clarity and accuracy.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWqxrtkRyDpx"
   },
   "outputs": [],
   "source": [
    "# Display response\n",
    "print(response.choices[0].message.content.strip())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
