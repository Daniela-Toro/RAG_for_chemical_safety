{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCzcrsPEaiu6"
   },
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuB2WORaaQKV"
   },
   "outputs": [],
   "source": [
    "# Install Dependencies (Colab only)\n",
    "!pip install -q unstructured[local-inference]\n",
    "!pip install -q langchain langchain-community langchain-openai chromadb openpyxl langchain_chroma\n",
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1skKQEAbakdy"
   },
   "source": [
    "# Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTD6OTpbaeRJ"
   },
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "from google.colab import userdata\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Font, Alignment\n",
    "import pdfplumber\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# LangChain Imports\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LangChain Community/OpenAI Extensions\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma as CommunityChroma\n",
    "from langchain_community.chat_models import ChatOpenAI as CommunityChatOpenAI\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFDirectoryLoader, TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings as OpenAIEmbeddingsV2, ChatOpenAI as ChatOpenAIV2\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma as NewChroma\n",
    "\n",
    "# Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHYOwuNebnxa"
   },
   "source": [
    "# File loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le1cR4SxbojL"
   },
   "outputs": [],
   "source": [
    "# Storage paths\n",
    "DB_Chroma = \"/content/drive/MyDrive/Grupo_1_RAG_Chemical_Safety/Notebooks/Chroma_DB/\"\n",
    "template_path = \"/content/drive/MyDrive/Grupo_1_RAG_Chemical_Safety/Notebooks/CO-028296-HS-2-COSHH template.xlsx\"\n",
    "output_Excel = \"/content/drive/MyDrive/Grupo_1_RAG_Chemical_Safety/Notebooks/output_Excel/\"\n",
    "json_excel = \"/content/drive/MyDrive/Grupo_1_RAG_Chemical_Safety/Notebooks/output_JSON/\"\n",
    "folder_documents = \"/content/drive/MyDrive/Grupo_1_RAG_Chemical_Safety/Notebooks/output_md_openai/\"\n",
    "\n",
    "# Load JSON paths\n",
    "json_table_Hazards_path = \"/content/drive/MyDrive/Grupo_1_RAG_Chemical_Safety/Notebooks/output_JSON/json_table_Hazards.json\"\n",
    "json_table_Waste_disposal_measures_path = \"/content/drive/MyDrive/Grupo_1_RAG_Chemical_Safety/Notebooks/output_JSON/json_table_Waste_disposal_measures.json\"\n",
    "json_table_Spill_management_path = \"/content/drive/MyDrive/Grupo_1_RAG_Chemical_Safety/Notebooks/output_JSON/json_table_Spill_management.json\"\n",
    "json_table_Fire_procedures_path = \"/content/drive/MyDrive/Grupo_1_RAG_Chemical_Safety/Notebooks/output_JSON/json_table_Fire_procedures.json\"\n",
    "json_table_First_aid_procedures_path = \"/content/drive/MyDrive/Grupo_1_RAG_Chemical_Safety/Notebooks/output_JSON/json_table_First_aid_procedures.json\"\n",
    "json_table_Storage_path = \"/content/drive/MyDrive/Grupo_1_RAG_Chemical_Safety/Notebooks/output_JSON/json_table_Storage.json\"\n",
    "\n",
    "# Load JSON tables\n",
    "with open(json_table_Hazards_path, 'r') as f:\n",
    "    json_hazards = json.load(f)\n",
    "\n",
    "with open(json_table_Waste_disposal_measures_path, 'r') as f:\n",
    "    json_waste_disposal_measures = json.load(f)\n",
    "\n",
    "with open(json_table_Spill_management_path, 'r') as f:\n",
    "    json_spill_management = json.load(f)\n",
    "\n",
    "with open(json_table_Fire_procedures_path, 'r') as f:\n",
    "    json_fire_procedures = json.load(f)\n",
    "\n",
    "with open(json_table_First_aid_procedures_path, 'r') as f:\n",
    "    json_first_aid_procedures = json.load(f)\n",
    "\n",
    "with open(json_table_Storage_path, 'r') as f:\n",
    "    json_storage = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0VWbbpfcGGA"
   },
   "source": [
    "# Embeddings, Vector DB, and LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yklYCw2c9ji6"
   },
   "outputs": [],
   "source": [
    "API_KEY = \"\"\n",
    "\n",
    "# Load GPT-4 family compatible embedding\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    api_key= API_KEY\n",
    "    )\n",
    "# Ensure the directory exists before loading the database\n",
    "if not os.path.exists(DB_Chroma):\n",
    "    os.makedirs(DB_Chroma)\n",
    "\n",
    "# Load Chroma DB\n",
    "db = Chroma(persist_directory=DB_Chroma, embedding_function=embeddings)\n",
    "print(chromadb.__version__)\n",
    "# Load the LLM\n",
    "llm = ChatOpenAI (\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg3nAxsyaR6E"
   },
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYgiYOHllNDc"
   },
   "outputs": [],
   "source": [
    "def list_db_sources(db):\n",
    "    \"\"\"\n",
    "    Lists all unique 'source' entries present in a Chroma database.\n",
    "    Args:\n",
    "        db: Chroma database instance.\n",
    "    Returns:\n",
    "        A set of unique source strings extracted from the database metadata.\n",
    "    Notes:\n",
    "        - Prints the total number of unique sources found.\n",
    "        - Prints up to the first 100 sources for quick inspection.\n",
    "    \"\"\"\n",
    "\n",
    "    res = db.get()\n",
    "    metadatas = res.get(\"metadatas\", [])\n",
    "    sources = {meta.get(\"source\", \"\") for meta in metadatas if isinstance(meta, dict)}\n",
    "    print(f\"Found {len(sources)} unique sources\")\n",
    "    for s in list(sources)[:100]:\n",
    "        print(\" -\", repr(s))\n",
    "    return sources\n",
    "\n",
    "list_db_sources(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwINJAvMiZsj"
   },
   "outputs": [],
   "source": [
    "query = \"WD 40\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aN4qXGaPbjrT"
   },
   "outputs": [],
   "source": [
    "# Function responsible for returning a document for the retriever (with content)\n",
    "def filter_document(query_doc, db, k=10):\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant document from a vector database based on a query,\n",
    "    and returns its content along with the source filename.\n",
    "    Args:\n",
    "        query_doc (str): The query text used to find similar documents.\n",
    "        db: Vector database object with an `as_retriever` method.\n",
    "        k (int, optional): Number of similar documents to fetch. Defaults to 10.\n",
    "    Returns:\n",
    "        tuple: (source_match, content)\n",
    "            - source_match (str): The filename of the most similar document.\n",
    "            - content (str or None): The full text content of the document, or None if\n",
    "              the file cannot be found or read.\n",
    "    Raises:\n",
    "        ValueError: If no similar documents are found in the database.\n",
    "\n",
    "    Notes:\n",
    "        - Uses the database retriever with similarity search.\n",
    "        - Assumes that the file exists in `folder_documents` with the name in metadata.\n",
    "        - If the physical file is missing or cannot be read, returns None for content.\n",
    "    \"\"\"\n",
    "\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    relevant_documents = retriever.invoke(query_doc)\n",
    "\n",
    "    if not relevant_documents:\n",
    "        raise ValueError(\"No similar document was found in the database.\")\n",
    "\n",
    "    # Full name of the document (according to metadata)\n",
    "    source_match = relevant_documents[0].metadata.get('source')\n",
    "    print(f\"Most similar document: {source_match}\")\n",
    "\n",
    "    # Physical path to the file\n",
    "    doc_path = os.path.join(folder_documents, source_match)\n",
    "\n",
    "    if not os.path.exists(doc_path):\n",
    "        print(f\"Physical file not found: {doc_path}\")\n",
    "        return source_match, None\n",
    "\n",
    "    try:\n",
    "        with open(doc_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "        return source_match, content\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the document: {e}\")\n",
    "        return source_match, None\n",
    "\n",
    "source_match, content = filter_document(query, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9iLl8xMcxDI"
   },
   "source": [
    "# Chemical Name and SDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuImxZgBV7b9"
   },
   "outputs": [],
   "source": [
    "# Function to get the document ID\n",
    "def get_document_id(source_match: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a base document ID from the given source string.\n",
    "    Args:\n",
    "        source_match (str): The source string (e.g., filename or document identifier).\n",
    "    Returns:\n",
    "        str: The first 14 characters of the source string, used as a base document ID.\n",
    "    Notes:\n",
    "        - Prints the extracted base ID for verification.\n",
    "    \"\"\"\n",
    "    # Extract the first 14 characters\n",
    "    base_id = source_match[:14]\n",
    "    print(f\"Extracted base ID: {base_id}\")\n",
    "    return base_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkqjervNV7ZX"
   },
   "outputs": [],
   "source": [
    "# Function to get the product name\n",
    "def get_product_name(source_match: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the product name from a given source string, typically a file name.\n",
    "    Args:\n",
    "        source_match (str): The source string or file name.\n",
    "    Returns:\n",
    "        str: The extracted product name, ignoring the first 15 characters and the last 4\n",
    "             characters (typically the file extension).\n",
    "    Notes:\n",
    "        - Prints the extracted product name for verification.\n",
    "        - Assumes the file name is at least 19 characters long (otherwise slicing may produce unexpected results).\n",
    "    \"\"\"\n",
    "\n",
    "    # Slice from character 15 to 3 characters from the end\n",
    "    product_name = source_match[15:-3]\n",
    "    print(f\"Product name: {product_name}\")\n",
    "    return product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ki8ftqy4V7Wg"
   },
   "outputs": [],
   "source": [
    "# Calling the functions: filter_document, get_document_id, get_product_name\n",
    "\n",
    "# Get the base ID from the file name\n",
    "base_id = get_document_id(source_match)\n",
    "\n",
    "# Get the product name from the file name\n",
    "product_name = get_product_name(source_match)\n",
    "\n",
    "# Results:\n",
    "print(\"source_match:\", source_match)\n",
    "print(\"base_id:\", base_id)\n",
    "print(\"product_name:\", product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Bdf_yssV7T8"
   },
   "outputs": [],
   "source": [
    "# Clean raw chemical names to remove noise\n",
    "def clean_chemical_names(raw_list):\n",
    "    \"\"\"\n",
    "    Cleans a list of raw chemical names by removing noise, irrelevant entries, and duplicates.\n",
    "    Args:\n",
    "        raw_list (list of str): Raw chemical names extracted from documents or databases.\n",
    "    Returns:\n",
    "        list of str: A list of cleaned chemical names.\n",
    "    Cleaning rules:\n",
    "        - Trims leading and trailing spaces, capitalizes words.\n",
    "        - Ignores names shorter than 4 or longer than 60 characters.\n",
    "        - Filters out names containing common noise words like 'No Substance', 'Regulation', 'Explosives', etc.\n",
    "        - Removes leading punctuation or asterisks.\n",
    "        - Collapses multiple spaces into a single space.\n",
    "        - Keeps only alphanumeric characters, spaces, hyphens, parentheses, and commas.\n",
    "        - Removes duplicates while preserving order.\n",
    "    \"\"\"\n",
    "\n",
    "    clean_names = []\n",
    "    noisy_words = [\n",
    "        \"No Substance\", \"Regulation\", \"Annex\", \"List\", \"Assessed\", \"Authorisation\",\n",
    "        \"Candidate\", \"Pop\", \"Pic\", \"Explosives\", \"Drug\", \"Ozone\", \"###\"\n",
    "    ]\n",
    "    for name in raw_list:\n",
    "        name = name.strip().title()\n",
    "        if len(name) < 4 or len(name) > 60:\n",
    "            continue\n",
    "        if any(word.lower() in name.lower() for word in noisy_words):\n",
    "            continue\n",
    "        name = re.sub(r\"^\\*\\*[:\\-]?\\s*\", \"\", name)\n",
    "        name = re.sub(r\"\\s{2,}\", \" \", name)\n",
    "        if not re.match(r'^[A-Za-z0-9\\s\\-\\(\\),]+$', name):\n",
    "            continue\n",
    "        if name not in clean_names:\n",
    "            clean_names.append(name)\n",
    "    return clean_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFmKPSezFI8b"
   },
   "outputs": [],
   "source": [
    "# Extract chemical names from a document (SDS/MSDS)\n",
    "def extract_chemical_names(source_match, content, use_llm=True, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Extracts chemical ingredient names from an SDS/MSDS document using a combination of regex and LLM-based parsing.\n",
    "    Args:\n",
    "        source_match (str): File name or identifier of the SDS/MSDS document.\n",
    "        content (str): Full text content of the SDS/MSDS document.\n",
    "        use_llm (bool): If True, uses an LLM to supplement extraction when regex finds few or no names.\n",
    "        model (str): Name of the OpenAI model to use if LLM extraction is enabled.\n",
    "    Returns:\n",
    "        list of str: Cleaned list of extracted chemical names.\n",
    "    Extraction procedure:\n",
    "        1. Uses regex to search for Section 3 (Composition / Ingredients) and extracts candidate names.\n",
    "        2. Optionally queries an LLM if regex finds fewer than two names.\n",
    "           - The LLM is prompted to return JSON containing only the chemical names.\n",
    "        3. Performs final cleaning:\n",
    "           - Removes short (<3) or long (>80) names.\n",
    "           - Filters out noisy or irrelevant words like \"Not Hazardous\", \"See Section\", etc.\n",
    "           - Removes duplicate names and normalizes capitalization.\n",
    "    \"\"\"\n",
    "\n",
    "    found_names = []\n",
    "\n",
    "    # Step 1: Regex on Section 3\n",
    "    section3 = \"\"\n",
    "    match = re.search(r\"(section\\s*3.*?composition.*?)(section\\s*\\d+|$)\", content, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        section3 = match.group(1)\n",
    "        patterns = [\n",
    "            r\"ingredient[s]?:?\\s*([\\w\\s\\-\\(\\)\\/]+)\",\n",
    "            r\"component[s]?:?\\s*([\\w\\s\\-\\(\\)\\/]+)\",\n",
    "            r\"substance\\s*name:?\\s*([\\w\\s\\-\\(\\)\\/]+)\"\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, section3, re.IGNORECASE)\n",
    "            for name in matches:\n",
    "                name = name.strip().title()\n",
    "                if name and name not in found_names:\n",
    "                    found_names.append(name)\n",
    "\n",
    "    # Step 2: LLM if regex is insufficient\n",
    "    if use_llm and (not found_names or len(found_names) < 2):\n",
    "        prompt_template = f\"\"\"\n",
    "    You are an expert assistant for analyzing chemical safety datasheets (SDS / MSDS).\n",
    "\n",
    "    Task: Extract **only the chemical ingredient names** from the following document.\n",
    "\n",
    "    Strict rules:\n",
    "    - Focus on Section 3 (Composition / Information on Ingredients)\n",
    "    - Ignore the general Product Name (e.g., WD-40 Multi-Use Product Aerosol)\n",
    "    - Extract only **individual chemical names**\n",
    "    - DO NOT include percentages, CAS numbers, regulatory phrases, or comments\n",
    "    - ALWAYS return valid JSON in this schema:\n",
    "\n",
    "    {{ \"chemical_names\": [\"Name1\", \"Name2\", \"Name3\"] }}\n",
    "\n",
    "    Document:\n",
    "    ---\n",
    "    {content}\n",
    "    ---\n",
    "    \"\"\"\n",
    "        llm = ChatOpenAI(model=model, temperature=0, openai_api_key=API_KEY)\n",
    "        response = llm.invoke(prompt_template).content\n",
    "\n",
    "        # Clean ```json ... ```\n",
    "        clean_response = response.strip().strip(\"`\").replace(\"json\", \"\").strip()\n",
    "\n",
    "        try:\n",
    "            data = json.loads(clean_response)\n",
    "            llm_names = data.get(\"chemical_names\", [])\n",
    "            for name in llm_names:\n",
    "                name = name.strip().title()\n",
    "                if name and name not in found_names:\n",
    "                    found_names.append(name)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not parse LLM response: {e}\")\n",
    "            print(\"Raw LLM response:\", response)\n",
    "\n",
    "    # Step 3: Final cleaning\n",
    "    final_names = []\n",
    "    noisy_words_final = [\"Not Hazardous\", \"No Substance\", \"See Section\", \"###\", \"Ltd\", \"Com\"]\n",
    "    for name in found_names:\n",
    "        if len(name) < 3 or len(name) > 80:\n",
    "            continue\n",
    "        if any(word.lower() in name.lower() for word in noisy_words_final):\n",
    "            continue\n",
    "        name = re.sub(r\"\\s{2,}\", \" \", name).strip()\n",
    "        if name not in final_names:\n",
    "            final_names.append(name)\n",
    "\n",
    "    print(f\"Document used: {source_match}\")\n",
    "    print(f\"Final chemical names: {final_names}\")\n",
    "    return final_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEvmpBcUV7RS"
   },
   "outputs": [],
   "source": [
    "# Call the function to get the chemical names from the compound\n",
    "chemical_names = extract_chemical_names(source_match, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neDb018FV7Or"
   },
   "outputs": [],
   "source": [
    "# Function to combine the product name with its list of chemical compounds\n",
    "def extract_product_info(source_match: str, content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts and combines the product name with its chemical ingredients from an SDS/MSDS document.\n",
    "    Args:\n",
    "        source_match (str): File name or identifier of the SDS/MSDS document.\n",
    "        content (str): Full text content of the SDS/MSDS document.\n",
    "    Returns:\n",
    "        dict: Dictionary containing:\n",
    "            - \"product_name\": the cleaned product name extracted from the file name.\n",
    "            - \"chemical_names\": a list of chemical compounds found in Section 3 of the SDS/MSDS.\n",
    "    Procedure:\n",
    "        1. Extract the product name from the file name, ignoring irrelevant prefixes or extensions.\n",
    "        2. Extract chemical names using `extract_chemical_names`, which uses regex and optionally an LLM.\n",
    "        3. Combine both pieces of information into a single dictionary for further processing or storage.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the base product name\n",
    "    product_name = get_product_name(source_match)\n",
    "\n",
    "    # Get chemical compounds\n",
    "    chemical_names = extract_chemical_names(source_match, content)\n",
    "\n",
    "    # Build dictionary\n",
    "    result = {\n",
    "        \"product_name\": product_name,\n",
    "        \"chemical_names\": chemical_names\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxZK6B5cV7MG"
   },
   "outputs": [],
   "source": [
    "# Calling the function: extract_product_info\n",
    "result = extract_product_info(source_match, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKTqX9WrODf7"
   },
   "outputs": [],
   "source": [
    "# Required fields for each data type\n",
    "hazards_fields = [\n",
    "    \"chemical_name\",\n",
    "    \"sds_reference\"\n",
    "]\n",
    "\n",
    "waste_disposal_measures_fields = [\n",
    "    \"chemical_name\"\n",
    "]\n",
    "\n",
    "spill_management_fields = [\n",
    "    \"chemical_name\"\n",
    "]\n",
    "\n",
    "fire_procedures_fields = [\n",
    "    \"chemical_name\"\n",
    "]\n",
    "\n",
    "first_aid_procedures_fields = [\n",
    "    \"chemical_name\"\n",
    "]\n",
    "\n",
    "storage_fields = [\n",
    "    \"chemical_name\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8B9hq-9OGCb"
   },
   "outputs": [],
   "source": [
    "lista_tablas = [\n",
    "    \"Hazards\",\n",
    "    \"Waste disposal measures, disposal waste\",\n",
    "    \"Spill management, spills, information_and_details_about_Spill_management\",\n",
    "    \"Fire procedures, Fire Fighting Measures, information_and_details_about_Fire_procedures\",\n",
    "    \"First aid procedures, First Aid Measures\",\n",
    "    \"Storage, Safe Storage\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-q-2z32ORs1"
   },
   "outputs": [],
   "source": [
    "def fill_json_chemical_fields(\n",
    "    json_input: Dict[str, Any],\n",
    "    content: str,\n",
    "    base_id: str,\n",
    "    chemical_names: Optional[List[str]] = None,\n",
    "    source_match: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Searches and fills the 'chemical_name' and 'sds_reference' fields\n",
    "    within a nested JSON structure without modifying other fields.\n",
    "\n",
    "    Args:\n",
    "        json_input (Dict[str, Any]): The nested JSON structure to update.\n",
    "        content (str): Full text content of the SDS/MSDS document.\n",
    "        base_id (str): Unique identifier extracted from the document or file name.\n",
    "        chemical_names (Optional[List[str]]): List of chemical names to fill.\n",
    "            If not provided, they are extracted from the document.\n",
    "        source_match (Optional[str]): File name or identifier of the SDS/MSDS document,\n",
    "            used to extract the product name.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The same JSON structure with updated 'chemical_name'\n",
    "        and 'sds_reference' fields.\n",
    "\n",
    "    Procedure:\n",
    "        1. If `chemical_names` is not provided, extract them from the document using `extract_chemical_names`.\n",
    "        2. Extract the product name from `source_match` if available.\n",
    "        3. Recursively traverse the JSON structure:\n",
    "            - For each 'chemical_name' field, fill it with either:\n",
    "                \"{product_name}: name1, name2, ...\" or \"name1, name2, ...\" if product name is unavailable.\n",
    "            - For each 'sds_reference' field, fill it with `base_id`.\n",
    "        4. Preserve the original structure of the JSON for all other fields.\n",
    "    \"\"\"\n",
    "\n",
    "    if chemical_names is None:\n",
    "        chemical_names = extract_chemical_names(source_match, content)\n",
    "\n",
    "    product_name = get_product_name(source_match) if source_match else None\n",
    "\n",
    "    def update_fields(data):\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "\n",
    "                # 🔹 Store as plain string\n",
    "                if key == \"chemical_name\" and isinstance(value, dict):\n",
    "                    if product_name:\n",
    "                        chemicals_str = f\"{product_name}: {', '.join(chemical_names)}\"\n",
    "                        value[\"response\"] = chemicals_str\n",
    "                        value[\"to_excel\"] = chemicals_str\n",
    "                    else:\n",
    "                        chemicals_str = \", \".join(chemical_names)\n",
    "                        value[\"response\"] = chemicals_str\n",
    "                        value[\"to_excel\"] = chemicals_str\n",
    "\n",
    "                elif key == \"sds_reference\" and isinstance(value, dict):\n",
    "                    value[\"response\"] = base_id\n",
    "                    value[\"to_excel\"] = base_id\n",
    "\n",
    "                else:\n",
    "                    update_fields(value)\n",
    "\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                update_fields(item)\n",
    "\n",
    "    update_fields(json_input)\n",
    "    return json_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGV2RLcoOWa5"
   },
   "outputs": [],
   "source": [
    "updated_json_hazards = fill_json_chemical_fields(\n",
    "    json_input=json_hazards,\n",
    "    content=content,\n",
    "    base_id=base_id,\n",
    "    source_match=source_match\n",
    ")\n",
    "\n",
    "updated_json_waste_disposal_measures = fill_json_chemical_fields(\n",
    "    json_input=json_waste_disposal_measures,\n",
    "    content=content,\n",
    "    base_id=base_id,\n",
    "    source_match=source_match\n",
    ")\n",
    "\n",
    "updated_json_spill_management = fill_json_chemical_fields(\n",
    "    json_input=json_spill_management,\n",
    "    content=content,\n",
    "    base_id=base_id,\n",
    "    source_match=source_match\n",
    ")\n",
    "\n",
    "updated_json_fire_procedures = fill_json_chemical_fields(\n",
    "    json_input=json_fire_procedures,\n",
    "    content=content,\n",
    "    base_id=base_id,\n",
    "    source_match=source_match\n",
    ")\n",
    "\n",
    "updated_json_first_aid_procedures = fill_json_chemical_fields(\n",
    "    json_input=json_first_aid_procedures,\n",
    "    content=content,\n",
    "    base_id=base_id,\n",
    "    source_match=source_match\n",
    ")\n",
    "\n",
    "updated_json_storage = fill_json_chemical_fields(\n",
    "    json_input=json_storage,\n",
    "    content=content,\n",
    "    base_id=base_id,\n",
    "    source_match=source_match\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82TZ6HYTeHX2"
   },
   "source": [
    "# Control Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpcDGLEujm2H"
   },
   "outputs": [],
   "source": [
    "# Required fields for my data type\n",
    "hazards_fields = [\n",
    "    \"explosive\",\n",
    "    \"flammable\",\n",
    "    \"oxidising\",\n",
    "    \"gas_under_pressure\",\n",
    "    \"acute_toxicity\",\n",
    "    \"corrosive\",\n",
    "    \"health_hazard\",\n",
    "    \"serious_health_hazard\",\n",
    "    \"hazardous_to_the_environment\"\n",
    "]\n",
    "\n",
    "hazards_protection_measures_fields = [\n",
    "    \"wear_full_face_visor\",\n",
    "    \"box_goggles_must_be_worn\",\n",
    "    \"protective_gloves_must_be_worn\",\n",
    "    \"laboratory_coats_must_be_worn\",\n",
    "    \"use_local_exhaust_ventillation\",\n",
    "    \"no_open_flames\",\n",
    "    \"other_control_measures\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQgRXGCfjmuT"
   },
   "outputs": [],
   "source": [
    "# Patterns by PPE field (for cleaning and exclusion)\n",
    "\n",
    "_FIELD_PATTERNS = {\n",
    "    \"wear_full_face_visor\": [r\"\\bface\\s*shield\\b\", r\"\\bfull\\s*face\\s*visor\\b\"],\n",
    "    \"box_goggles_must_be_worn\": [r\"\\bgoggles\\b\", r\"\\bsafety\\s*glasses\\b\", r\"\\beye\\s*protection\\b\"],\n",
    "    \"protective_gloves_must_be_worn\": [r\"\\bprotective\\s*gloves\\b\", r\"\\bhand\\s*protection\\b\", r\"\\bgloves\\b\", r\"\\bnitrile\\b\", r\"\\bbutyl\\b\"],\n",
    "    \"laboratory_coats_must_be_worn\": [r\"\\blab\\s*coat\\b\", r\"\\bprotective\\s*clothing\\b\", r\"\\bbody\\s*protection\\b\"],\n",
    "    \"use_local_exhaust_ventillation\": [r\"\\blocal\\s*exhaust\\s*ventilation\\b\", r\"\\bLEV\\b\", r\"\\bfume\\s*hood\\b\"],\n",
    "    \"no_open_flames\": [r\"\\bno\\s*open\\s*flames\\b\", r\"\\bignition\\s*sources\\b\", r\"\\bkeep\\s*away\\s*from\\s*ignition\\b\", r\"\\bnon\\-?sparking\\s*tools?\\b\"],\n",
    "}\n",
    "\n",
    "PPE_FIELDS = [\n",
    "    \"wear_full_face_visor\",\n",
    "    \"box_goggles_must_be_worn\",\n",
    "    \"protective_gloves_must_be_worn\",\n",
    "    \"laboratory_coats_must_be_worn\",\n",
    "    \"use_local_exhaust_ventilation\",\n",
    "    \"no_open_flames\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oAf0J6Ljmwq"
   },
   "outputs": [],
   "source": [
    "def _candidate_lines(text: str):\n",
    "    \"\"\"\n",
    "    Extracts useful lines from a text document by cleaning and normalizing each line.\n",
    "    Args:\n",
    "        text (str): Raw text input, possibly containing bullets, leading/trailing whitespace,\n",
    "                    or other list symbols.\n",
    "    Returns:\n",
    "        list: A list of cleaned lines, stripped of leading/trailing whitespace and common bullet\n",
    "              or list markers, ready for further processing.\n",
    "    Procedure:\n",
    "        1. Split the text into lines.\n",
    "        2. Remove empty lines.\n",
    "        3. Strip leading/trailing whitespace from each line.\n",
    "        4. Remove common list markers or bullets such as '-', '*', '•', etc.\n",
    "        5. Return the resulting list of cleaned lines.\n",
    "    \"\"\"\n",
    "\n",
    "    lines = []\n",
    "    for raw_line in text.splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        line = re.sub(r\"^[\\-\\*\\•\\u2022]+\\s*\", \"\", line).strip()\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEb_JcAmjmsP"
   },
   "outputs": [],
   "source": [
    "def _matches_any(line: str, patterns):\n",
    "    \"\"\"\n",
    "    Checks whether a given text line matches any of the provided regular expression patterns.\n",
    "    Args:\n",
    "        line (str): The text line to check.\n",
    "        patterns (list): A list of regex patterns (strings) to match against the line.\n",
    "    Returns:\n",
    "        bool: True if at least one pattern matches the line, False otherwise.\n",
    "    Procedure:\n",
    "        1. Convert the line to lowercase for case-insensitive matching.\n",
    "        2. Iterate over all patterns.\n",
    "        3. Use `re.search` to check for a match with each pattern.\n",
    "        4. Return True on the first match found; otherwise, return False.\n",
    "    \"\"\"\n",
    "\n",
    "    line_lower = line.lower()\n",
    "    return any(re.search(p, line_lower) for p in patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUZoYeGskQZf"
   },
   "outputs": [],
   "source": [
    "def _find_support_for_field(summary_text: str, field_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds the first line in the provided text that matches the patterns associated with a given field.\n",
    "    Args:\n",
    "        summary_text (str): The text block or summary from which to extract supporting lines.\n",
    "        field_key (str): The key of the field for which we want supporting evidence.\n",
    "    Returns:\n",
    "        str: The first matching line supporting the field. If no match is found, returns an empty string.\n",
    "    Procedure:\n",
    "        1. Retrieve the regex patterns associated with `field_key` from `_FIELD_PATTERNS`.\n",
    "        2. If no patterns exist for this field, return an empty string.\n",
    "        3. Split the text into candidate lines using `_candidate_lines`.\n",
    "        4. Check each line against all patterns using `_matches_any`.\n",
    "        5. Return the first line that matches; if none match, return \"\".\n",
    "    \"\"\"\n",
    "\n",
    "    patterns = _FIELD_PATTERNS.get(field_key, [])\n",
    "    if not patterns:\n",
    "        return \"\"\n",
    "    for line in _candidate_lines(summary_text):\n",
    "        if _matches_any(line, patterns):\n",
    "            return line.strip()\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DsxBREW4YvP"
   },
   "outputs": [],
   "source": [
    "def _extract_json_block(text: str):\n",
    "    \"\"\"\n",
    "    Extracts a JSON block from a given text string.\n",
    "    Args:\n",
    "        text (str): The input text potentially containing JSON data.\n",
    "                    Supports JSON wrapped in ```json ... ``` or plain JSON.\n",
    "    Returns:\n",
    "        str: The JSON string extracted from the text. If no explicit JSON block is found,\n",
    "             returns the original text as a fallback.\n",
    "    Procedure:\n",
    "        1. Attempt to locate a JSON block using a regex that captures from '{' to '}' at the end of the text.\n",
    "        2. If a match is found, return the matched JSON string.\n",
    "        3. If no match is found, return the original text as a fallback for parsing.\n",
    "    \"\"\"\n",
    "\n",
    "    match = re.search(r\"\\{.*\\}\\s*$\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    # Plan B: try parsing as is\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdFDHrwpkQWc"
   },
   "outputs": [],
   "source": [
    "def control_measures_with_images(field_name, content, fields_list, data_dict, llm):\n",
    "    \"\"\"\n",
    "    Extracts control measures (including PPE) from a text context and fills the JSON accordingly.\n",
    "    Args:\n",
    "        field_name (str): Name of the field to extract (e.g., 'Hazard Statements').\n",
    "        content (str): Text content (e.g., SDS section) to analyze.\n",
    "        fields_list (list): List of JSON keys corresponding to PPE and other control measures.\n",
    "        data_dict (dict): The JSON structure where the results will be stored.\n",
    "                          Keys must match `fields_list` and contain 'to_excel' and 'response'.\n",
    "        llm: Language model object with an 'invoke' method for generating text.\n",
    "    Returns:\n",
    "        dict: Updated `data_dict` with the following behavior:\n",
    "            - PPE fields (6 standard items) marked with 'X' in 'to_excel' if present in the text.\n",
    "              Supporting line stored in 'response'.\n",
    "            - 'other_control_measures' stores any control measures not included in the PPE fields:\n",
    "                * 'to_excel': semi-colon joined short phrases\n",
    "                * 'response': original paragraph(s) from context or the list if paragraph empty\n",
    "    Behavior:\n",
    "        1. Generates a base summary from the content strictly using the text provided.\n",
    "        2. Checks and marks six standard PPE fields with 'X' if mentioned or implied.\n",
    "        3. Extracts all other control measures not in the PPE fields in a structured JSON:\n",
    "           { \"list\": [...], \"paragraph\": \"...\" }.\n",
    "        4. Writes these results into the provided JSON structure for Excel export.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Base summary: which risks/measures appear, without inventing anything\n",
    "    base_prompt = \"\"\"\n",
    "    Answer STRICTLY using only the content retrieved from the provided context.\n",
    "    Do not invent or add external information.\n",
    "    If the context contains no information relevant to the question, state explicitly that the information is not available.\n",
    "    \"\"\"\n",
    "    request = f\"\"\"\n",
    "    Answer the question based only on these instructions: {base_prompt}.\n",
    "    What are the main {field_name} risks or measures in the context: {content}?\n",
    "    Answer in bullet points, keeping the exact wording from the context whenever possible.\n",
    "    \"\"\"\n",
    "    field_summary = llm.invoke(request).content\n",
    "\n",
    "    # Special case: Hazard Statements\n",
    "    if field_name == 'Hazard Statements' and \"hazard_statements\" in data_dict:\n",
    "        data_dict[\"hazard_statements\"][\"to_excel\"] = field_summary.replace('*','')\n",
    "\n",
    "    # 2) Prompt PPE: binary mapping of 6 fields\n",
    "    mapping_prompt = f\"\"\"\n",
    "    Based only on this extracted information:\n",
    "    {field_summary}\n",
    "\n",
    "    Check which of the following protection measures are explicitly required or implied.\n",
    "    Mark with 'X' if true, otherwise '' (empty string).\n",
    "\n",
    "    Fields:\n",
    "    - wear_full_face_visor: full face visor, face shield\n",
    "    - box_goggles_must_be_worn: eye protection, goggles, safety glasses\n",
    "    - protective_gloves_must_be_worn: protective gloves, hand protection\n",
    "    - laboratory_coats_must_be_worn: lab coat, protective clothing, body protection\n",
    "    - use_local_exhaust_ventilation: local exhaust ventilation, fume hood\n",
    "    - no_open_flames: no open flames, keep away from ignition sources\n",
    "\n",
    "    Respond EXACTLY with lines like:\n",
    "    field_name: X\n",
    "    field_name:\n",
    "    (one per line; no extra commentary)\n",
    "    \"\"\"\n",
    "    ppe_result = llm.invoke(mapping_prompt).content\n",
    "\n",
    "    # Initialize PPE fields empty\n",
    "    for field in fields_list:\n",
    "        data_dict[field][\"to_excel\"] = \"\"\n",
    "        data_dict[field][\"response\"] = \"\"\n",
    "\n",
    "    # Mark PPE fields and add evidence\n",
    "    for line in ppe_result.splitlines():\n",
    "        if \":\" not in line:\n",
    "            continue\n",
    "        field, value = line.split(\":\", 1)\n",
    "        field = field.strip().lstrip(\"-\").strip()\n",
    "        value = value.strip()\n",
    "        if field in fields_list and field != \"other_control_measures\":\n",
    "            if value == \"X\":\n",
    "                data_dict[field][\"to_excel\"] = \"X\"\n",
    "                data_dict[field][\"response\"] = _find_support_for_field(field_summary, field) or \"\"\n",
    "\n",
    "    # 3) Prompt \"Other control measures\" (excluding the six PPE fields)\n",
    "    active_ppe = [f for f in PPE_FIELDS if data_dict.get(f, {}).get(\"to_excel\") == \"X\"]\n",
    "    other_prompt = f\"\"\"\n",
    "    You are given this extracted text (context):\n",
    "    {field_summary}\n",
    "\n",
    "    Task: List ALL explicit control or prevention measures that are NOT any of these categories:\n",
    "    - full face visor / face shield\n",
    "    - eye protection / goggles / safety glasses\n",
    "    - protective gloves / hand protection\n",
    "    - lab coat / protective clothing / body protection\n",
    "    - local exhaust ventilation / fume hood\n",
    "    - no open flames / ignition sources\n",
    "\n",
    "    Also, avoid simply rephrasing measures already covered by these categories, even if they are active: {active_ppe}.\n",
    "\n",
    "    Return STRICT JSON with this schema:\n",
    "    {{\n",
    "      \"list\": [\"short, practical measure 1\", \"short, practical measure 2\", ...],\n",
    "      \"paragraph\": \"exact paragraph(s) from the context where those 'other' measures appear\"\n",
    "    }}\n",
    "\n",
    "    Requirements:\n",
    "    - \"list\": concise, actionable phrases; do not include H-codes or P-codes; no headings like 'Hygiene:'.\n",
    "    - \"paragraph\": copy the original text fragment(s) verbatim from the context containing those 'other' measures.\n",
    "    - If there are none, return: {{ \"list\": [], \"paragraph\": \"\" }}.\n",
    "    \"\"\"\n",
    "    other_raw = llm.invoke(other_prompt).content\n",
    "\n",
    "    # Robust JSON parsing\n",
    "    other_json = {\"list\": [], \"paragraph\": \"\"}\n",
    "    try:\n",
    "        block = _extract_json_block(other_raw)\n",
    "        other_json = json.loads(block)\n",
    "        if not isinstance(other_json, dict):\n",
    "            other_json = {\"list\": [], \"paragraph\": \"\"}\n",
    "    except Exception:\n",
    "        other_json = {\"list\": [], \"paragraph\": \"\"}\n",
    "\n",
    "    # Normalize types\n",
    "    measures_list = other_json.get(\"list\", [])\n",
    "    if not isinstance(measures_list, list):\n",
    "        measures_list = []\n",
    "    paragraph = other_json.get(\"paragraph\", \"\")\n",
    "    if not isinstance(paragraph, str):\n",
    "        paragraph = \"\"\n",
    "\n",
    "    # Write other_control_measures\n",
    "    if \"other_control_measures\" in fields_list:\n",
    "        data_dict[\"other_control_measures\"][\"to_excel\"] = \" \".join(measures_list).strip(\" ;\")\n",
    "        # response = original paragraph; if empty, use joined list\n",
    "        data_dict[\"other_control_measures\"][\"response\"] = paragraph if paragraph.strip() else \"; \".join(measures_list).strip(\" ;\")\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELA_aI1VjmiL"
   },
   "outputs": [],
   "source": [
    "field_name = 'Personal Protection'\n",
    "updated_json_hazards = control_measures_with_images(\n",
    "    field_name,\n",
    "    content,\n",
    "    hazards_protection_measures_fields,\n",
    "    json_hazards['Sheet_2'],\n",
    "    llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dIjAeZDeGir"
   },
   "source": [
    "# Hazards Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2Mq-5sUL5DM"
   },
   "outputs": [],
   "source": [
    "hazards_fields_migue = [\n",
    "    \"explosive\",\n",
    "    \"flammable\",\n",
    "    \"oxidising\",\n",
    "    \"gas_under_pressure\",\n",
    "    \"acute_toxicity\",\n",
    "    \"corrosive\",\n",
    "    \"health_hazard\",\n",
    "    \"serious_health_hazard\",\n",
    "    \"hazardous_to_the_environment\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbyEDLqoaeVL"
   },
   "outputs": [],
   "source": [
    "def fields_with_images(field_name, content, fields_list, data_dict, llm):\n",
    "    \"\"\"\n",
    "    Updates hazard/pictogram fields in a JSON based on LLM analysis.\n",
    "    Args:\n",
    "        field_name (str): Name of the main field being processed (e.g., 'Hazard Statements').\n",
    "        content (str): Text context from which hazards and measures are extracted.\n",
    "        fields_list (list): List of keys in `data_dict` corresponding to hazard/pictogram fields.\n",
    "        data_dict (dict): JSON structure where the results will be stored.\n",
    "                          Each key must contain 'content', 'position', and 'to_excel'.\n",
    "        llm: Language model object with an 'invoke' or 'predict' method to get LLM responses.\n",
    "    Behavior:\n",
    "        1. Generates a base response from the LLM describing the main risks/measures in the context.\n",
    "        2. If `field_name` is 'Hazard Statements', updates the `hazard_statements` entry in `to_excel`.\n",
    "        3. Iterates over each field in `fields_list` (pictograms/hazard indicators):\n",
    "            - Prompts the LLM to check if the risk is explicitly mentioned.\n",
    "            - Marks 'X' in `to_excel` if present, otherwise leaves it empty.\n",
    "        4. Updates the original `data_dict` with the new 'to_excel' values for all fields.\n",
    "    Returns:\n",
    "        None: Updates `data_dict` in place.\n",
    "    \"\"\"\n",
    "\n",
    "    update_dict = {}\n",
    "\n",
    "    base_prompt = f\"\"\"\n",
    "    Answer STRICTLY using only the content retrieved from the provided context.\n",
    "    Do not invent or add external information.\n",
    "    If the context contains no information relevant to the question, state explicitly that the information is not available.\n",
    "    \"\"\"\n",
    "    request = f\"\"\"\n",
    "    Answer the question based only on these instructions: {base_prompt}.\n",
    "    What are the main {field_name} risks or measures in the context: {content}?\n",
    "    Answer only the explicit values and exclude other precautions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get base response from LLM\n",
    "    field_response = llm.invoke(request).content\n",
    "\n",
    "    # Save hazard statements text if applicable\n",
    "    if field_name == 'Hazard Statements':\n",
    "        update_dict[\"hazard_statements\"] = {\n",
    "            'content': data_dict[\"hazard_statements\"][\"content\"],\n",
    "            'position': data_dict[\"hazard_statements\"][\"position\"],\n",
    "            'to_excel': field_response.replace('*','').replace('#','')\n",
    "        }\n",
    "\n",
    "    print(\"Base field response:\", field_response, \"\\n\")\n",
    "\n",
    "    # Iterate over each pictogram/hazard field and mark with 'X' if applicable\n",
    "    for field in fields_list:\n",
    "        request_images = f\"\"\"\n",
    "        Answer the question based only on this information: {field_response}.\n",
    "        In the provided context, is there a {data_dict[field][\"content\"]} risk explicitly mentioned?\n",
    "        Answer only 'X' if True, else '' (empty string).\n",
    "        For the Serious health hazard risk, answer 'X' only if there is an extreme danger.\n",
    "        \"\"\"\n",
    "\n",
    "        to_excel_value = llm.predict(request_images)\n",
    "        print(data_dict[field][\"content\"], to_excel_value)\n",
    "\n",
    "        # Update dictionary\n",
    "        update_dict[field] = {\n",
    "            'content': data_dict[field][\"content\"],\n",
    "            'position': data_dict[field][\"position\"],\n",
    "            'to_excel': to_excel_value\n",
    "        }\n",
    "\n",
    "    # Finally, write the responses into the original dictionary\n",
    "    for field, value in update_dict.items():\n",
    "        data_dict[field]['to_excel'] = value['to_excel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tt_3PJuiahi8"
   },
   "outputs": [],
   "source": [
    "fields_with_images(\n",
    "    field_name='Hazard Statements',\n",
    "    content=content,\n",
    "    fields_list=hazards_fields_migue,\n",
    "    data_dict=updated_json_hazards,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itb1KwQ1jMUD"
   },
   "source": [
    "# Storage Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGFlqWdER2Uu"
   },
   "outputs": [],
   "source": [
    "# STORAGE Fields\n",
    "STORAGE_FIELDS = [\n",
    "    \"flammables_cupboard\",\n",
    "    \"corrosives_cupboard\",\n",
    "    \"poisons_cupboard\",\n",
    "    \"ventilated_storage\",\n",
    "    \"gas_cylinder\",\n",
    "    \"cold_storage\",\n",
    "    \"dessicated_storage\",\n",
    "]\n",
    "\n",
    "# Patterns by field (to search for evidence in the base text)\n",
    "_STORAGE_PATTERNS = {\n",
    "    \"flammables_cupboard\": [\n",
    "        r\"\\bflammable(s)?\\b\", r\"\\bflammables\\s*cupboard\\b\",\n",
    "        r\"\\bstore in flammables?( cabinet| cupboard)?\\b\",\n",
    "        r\"\\bkeep away from (heat|open flames|ignition sources)\\b\",\n",
    "    ],\n",
    "    \"corrosives_cupboard\": [\n",
    "        r\"\\bcorrosive(s)?\\b\", r\"\\bcorrosives?\\s*(cupboard|cabinet)\\b\",\n",
    "        r\"\\bstore in (a )?corrosives? (cabinet|cupboard)\\b\",\n",
    "        r\"\\bacids?\\b\", r\"\\bbases?\\b\",\n",
    "    ],\n",
    "    \"poisons_cupboard\": [\n",
    "        r\"\\bpoison(s|ous)?\\b\", r\"\\btox(ic|icity)\\b\",\n",
    "        r\"\\bpoisons?\\s*(cupboard|cabinet)\\b\",\n",
    "        r\"\\bstore in locked (cabinet|cupboard)\\b\",\n",
    "    ],\n",
    "    \"ventilated_storage\": [\n",
    "        r\"\\bventilated storage\\b\", r\"\\bventilated area\\b\",\n",
    "        r\"\\bstore in a well-ventilated place\\b\", r\"\\bKEEP CONTAINER TIGHTLY CLOSED IN A WELL-VENTILATED PLACE\\b\",\n",
    "        r\"\\blocal exhaust\\b\", r\"\\bfume hood\\b\",\n",
    "    ],\n",
    "    \"gas_cylinder\": [\n",
    "        r\"\\bgas cylinder(s)?\\b\", r\"\\bcompressed gas(es)?\\b\", r\"\\bpressurized\\b\",\n",
    "        r\"\\bsecure cylinder(s)?\\b\", r\"\\bupright\\b\", r\"\\bcap(s)? in place\\b\",\n",
    "    ],\n",
    "    \"cold_storage\": [\n",
    "        r\"\\bcold storage\\b\", r\"\\brefrigerate(d)?\\b\", r\"\\bstore (at|below) \\d+ ?°?C\\b\",\n",
    "        r\"\\btemperature control\\b\",\n",
    "    ],\n",
    "    \"dessicated_storage\": [\n",
    "        r\"\\bdessicat(ed|ion)?\\b\", r\"\\bdesiccator\\b\", r\"\\bdry storage\\b\",\n",
    "        r\"\\bkeep dry\\b\", r\"\\bprotect from moisture\\b\", r\"\\bmoisture sensitive\\b\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGXGcw5alCTp"
   },
   "outputs": [],
   "source": [
    "def _candidate_lines(text: str):\n",
    "    \"\"\"\n",
    "    Cleans and extracts candidate lines from a block of text.\n",
    "    This function:\n",
    "    - Removes leading/trailing whitespace.\n",
    "    - Strips common bullet or list symbols (e.g., '-', '*', '•').\n",
    "    - Ignores empty lines.\n",
    "    - Returns a list of cleaned lines suitable for further processing,\n",
    "      such as pattern matching or data extraction.\n",
    "    Args:\n",
    "        text (str): The raw text block to process.\n",
    "    Returns:\n",
    "        list[str]: A list of cleaned, non-empty lines.\n",
    "    \"\"\"\n",
    "\n",
    "    lines = []\n",
    "    for raw_line in text.splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        line = re.sub(r\"^[\\-\\*\\•\\u2022]+\\s*\", \"\", line).strip()\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ACNkkre5l_q"
   },
   "outputs": [],
   "source": [
    "def _matches_any(line: str, patterns):\n",
    "    \"\"\"\n",
    "    Determines whether a given line of text matches any of the provided regex patterns.\n",
    "    Args:\n",
    "        line (str): A single line of text to be checked.\n",
    "        patterns (list[str]): A list of regular expression patterns to match against.\n",
    "    Returns:\n",
    "        bool: True if the line matches at least one pattern, False otherwise.\n",
    "    Notes:\n",
    "        - The match is case-insensitive since the line is converted to lowercase.\n",
    "        - Useful for filtering or identifying relevant lines in text extraction tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    line_lower = line.lower()\n",
    "    return any(re.search(p, line_lower) for p in patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GlZmyfK51bl"
   },
   "outputs": [],
   "source": [
    "def _find_support_for_storage(base_summary: str, field_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds supporting evidence for a specific storage-related field in a summary text.\n",
    "    Args:\n",
    "        base_summary (str): The text summary to search within.\n",
    "        field_key (str): The storage field key to find evidence for (used to look up regex patterns).\n",
    "    Returns:\n",
    "        str: The first line from the summary that matches any pattern associated with the field.\n",
    "             Returns an empty string if no match is found.\n",
    "    Notes:\n",
    "        - Uses _STORAGE_PATTERNS (a dictionary mapping field keys to regex patterns) for matching.\n",
    "        - Lines are preprocessed with _candidate_lines to remove bullets, whitespace, and list symbols.\n",
    "        - Matches are case-insensitive via _matches_any.\n",
    "    \"\"\"\n",
    "\n",
    "    patterns = _STORAGE_PATTERNS.get(field_key, [])\n",
    "    if not patterns:\n",
    "        return \"\"\n",
    "    for line in _candidate_lines(base_summary):\n",
    "        if _matches_any(line, patterns):\n",
    "            return line.strip()\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MUbsRKP56NC"
   },
   "outputs": [],
   "source": [
    "def _extract_json_block(text: str):\n",
    "    \"\"\"\n",
    "    Extracts a JSON block from a given text string.\n",
    "    Args:\n",
    "        text (str): Input text that may contain a JSON object, possibly wrapped in ```json ... ```.\n",
    "    Returns:\n",
    "        str: The JSON block as a string if found; otherwise, returns the original input text.\n",
    "    Notes:\n",
    "        - Uses a regular expression to detect the first valid JSON object at the end of the text.\n",
    "        - Supports multi-line JSON by using DOTALL mode.\n",
    "        - If no JSON object is detected, the function safely returns the original text.\n",
    "    \"\"\"\n",
    "\n",
    "    match = re.search(r\"\\{.*\\}\\s*$\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBgJttCh58qb"
   },
   "outputs": [],
   "source": [
    "def storage_fields_with_images(field_name, content, fields_list, data_dict, llm):\n",
    "    \"\"\"\n",
    "    Processes storage-related fields in an SDS/MSDS context.\n",
    "    - Step 1 (PPE-Storage): Marks 'X' in `to_excel` for 7 predefined STORAGE fields.\n",
    "      Stores the supporting line from the base summary in `response`.\n",
    "\n",
    "    - Step 2 (Other storage measures): Handles `special_storage_describe`.\n",
    "      Returns only storage measures that are not part of the 7 predefined fields, in JSON:\n",
    "        { \"list\": [...], \"paragraph\": \"...\" }\n",
    "      - `to_excel` = '; '.join(list)\n",
    "      - `response` = paragraph (or joined list if paragraph is empty)\n",
    "    Args:\n",
    "        field_name (str): Name of the storage section (e.g., \"Storage\").\n",
    "        content (str): Full extracted text from the document.\n",
    "        fields_list (list): List of JSON keys corresponding to storage fields.\n",
    "        data_dict (dict): JSON structure to update with `to_excel` and `response`.\n",
    "        llm: LLM object with a .invoke() method to query content.\n",
    "    Returns:\n",
    "        dict: Updated `data_dict` with populated storage fields.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Base summary\n",
    "    base_prompt = \"\"\"\n",
    "    Answer STRICTLY using only the content retrieved from the provided context.\n",
    "    Do not invent or add external information.\n",
    "    If the context contains no information relevant to the question, state explicitly that the information is not available.\n",
    "    \"\"\"\n",
    "    request = f\"\"\"\n",
    "    Answer the question based only on these instructions: {base_prompt}.\n",
    "    What are the main {field_name} storage requirements or recommendations in the context: {content}?\n",
    "    Answer in bullet points, keeping the exact wording from the context whenever possible.\n",
    "    \"\"\"\n",
    "\n",
    "    base_response = llm.invoke(request).content\n",
    "\n",
    "    # STORAGE prompt: binary mapping of 7 fields\n",
    "    mapping_prompt = f\"\"\"\n",
    "    Based only on this extracted information:\n",
    "    {base_response}\n",
    "\n",
    "    Check which of the following STORAGE requirements are explicitly required or implied.\n",
    "    Mark with 'X' if true, otherwise '' (empty string).\n",
    "\n",
    "    Fields:\n",
    "    - flammables_cupboard: store in flammables cabinet/cupboard; keep away from ignition sources/heat\n",
    "    - corrosives_cupboard: store in corrosives cabinet/cupboard; acids/bases segregation\n",
    "    - poisons_cupboard: store in poisons/toxics cabinet; locked storage\n",
    "    - ventilated_storage: ventilated storage, well-ventilated place, fume hood area\n",
    "    - gas_cylinder: gas cylinders handling/storage, upright, secured, caps on\n",
    "    - cold_storage: ONLY IF refrigeration or cold room is explicitly stated (e.g., \"refrigerate\", \"cold storage\", \"store at/below ≤10°C\", \"2–8°C\").\n",
    "                    NOT phrases like \"keep cool\", \"store in a cool, dry/well-ventilated place\".\n",
    "    - dessicated_storage: desiccator, dry storage, keep dry, protect from moisture\n",
    "\n",
    "    Respond EXACTLY with lines like:\n",
    "    field_name: X\n",
    "    field_name:\n",
    "    (one per line; no extra commentary)\n",
    "    \"\"\"\n",
    "    result = llm.invoke(mapping_prompt).content\n",
    "\n",
    "    # Initialize empty fields\n",
    "    for field in fields_list:\n",
    "        data_dict[field][\"to_excel\"] = \"\"\n",
    "        data_dict[field][\"response\"] = \"\"\n",
    "\n",
    "    # Mark 'X' and add supporting evidence\n",
    "    for line in result.splitlines():\n",
    "        if \":\" not in line:\n",
    "            continue\n",
    "        field, value = line.split(\":\", 1)\n",
    "        field = field.strip().lstrip(\"-\").strip()\n",
    "        value = value.strip()\n",
    "        if field in fields_list and field != \"special_storage_describe\":\n",
    "            if value == \"X\":\n",
    "                data_dict[field][\"to_excel\"] = \"X\"\n",
    "                data_dict[field][\"response\"] = _find_support_for_storage(base_response, field) or \"\"\n",
    "\n",
    "    # Prompt \"special_storage_describe\" (other storage measures)\n",
    "    active_fields = [f for f in STORAGE_FIELDS if data_dict.get(f, {}).get(\"to_excel\") == \"X\"]\n",
    "    other_prompt = f\"\"\"\n",
    "    You are given this extracted text (context):\n",
    "    {base_response}\n",
    "\n",
    "    Task: List ALL explicit storage measures that are NOT any of these categories:\n",
    "    - flammables cupboard (flammables cabinet/cupboard; ignition sources)\n",
    "    - corrosives cupboard (corrosives cabinet; acids/bases segregation)\n",
    "    - poisons cupboard (toxics cabinet; locked storage)\n",
    "    - ventilated storage (well-ventilated place; fume hood area)\n",
    "    - gas cylinder storage (upright; secured; caps on)\n",
    "    - cold storage (refrigerate; keep cool; temp control)\n",
    "    - dessicated storage (desiccator; keep dry; protect from moisture)\n",
    "\n",
    "    Also, avoid rephrasing measures already covered by active categories: {active_fields}.\n",
    "\n",
    "    Return STRICT JSON with this schema:\n",
    "    {{\n",
    "      \"list\": [\"short, practical storage measure 1\", \"short, practical storage measure 2\", ...],\n",
    "      \"paragraph\": \"exact paragraph(s) from the context where those 'other' storage measures appear\"\n",
    "    }}\n",
    "\n",
    "    Requirements:\n",
    "    - \"list\": concise, actionable phrases; do not include H-codes or P-codes; no generic headings.\n",
    "    - \"paragraph\": copy the original text fragment(s) verbatim from the context containing those 'other' storage measures.\n",
    "    - If there are none, return: {{ \"list\": [], \"paragraph\": \"\" }}.\n",
    "    \"\"\"\n",
    "    other_raw = llm.invoke(other_prompt).content\n",
    "\n",
    "    # Robust JSON parsing\n",
    "    other_json = {\"list\": [], \"paragraph\": \"\"}\n",
    "    try:\n",
    "        block = _extract_json_block(other_raw)\n",
    "        other_json = json.loads(block)\n",
    "        if not isinstance(other_json, dict):\n",
    "            other_json = {\"list\": [], \"paragraph\": \"\"}\n",
    "    except Exception:\n",
    "        other_json = {\"list\": [], \"paragraph\": \"\"}\n",
    "\n",
    "    measures_list = other_json.get(\"list\", [])\n",
    "    if not isinstance(measures_list, list):\n",
    "        measures_list = []\n",
    "    paragraph = other_json.get(\"paragraph\", \"\")\n",
    "    if not isinstance(paragraph, str):\n",
    "        paragraph = \"\"\n",
    "\n",
    "    # Write special_storage_describe\n",
    "    if \"special_storage_describe\" in fields_list:\n",
    "        data_dict[\"special_storage_describe\"][\"to_excel\"] = \"; \".join(measures_lis)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TVP0fB-6clC"
   },
   "outputs": [],
   "source": [
    "field_name = 'Storage'\n",
    "updated_json_Storage = storage_fields_with_images(\n",
    "    field_name,\n",
    "    content,\n",
    "    STORAGE_FIELDS,\n",
    "    json_storage['Sheet_2'],\n",
    "    llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDZgiq_VdWmh"
   },
   "source": [
    "# Hazard Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebry-r-HHlpD"
   },
   "outputs": [],
   "source": [
    "# Function to update the hazard group in a JSON structure\n",
    "def update_hazard_group_in_json(data, hazard_letter):\n",
    "    \"\"\"\n",
    "    Recursively traverses a nested JSON structure and updates all occurrences\n",
    "    of the 'hazard_group' field by setting its 'response' and 'to_excel' values.\n",
    "    Args:\n",
    "        data (dict or list): Nested JSON structure containing hazard fields.\n",
    "        hazard_letter (str): The hazard group letter (e.g., 'A', 'B', 'C') to assign.\n",
    "    Behavior:\n",
    "        - If a dictionary has a key 'hazard_group' and its value is a dictionary,\n",
    "          sets both 'response' and 'to_excel' to `hazard_letter`.\n",
    "        - Recursively processes nested dictionaries and lists.\n",
    "    Returns:\n",
    "        The updated JSON structure with all 'hazard_group' fields modified.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if key == \"hazard_group\" and isinstance(value, dict):\n",
    "                value[\"response\"] = hazard_letter\n",
    "                value[\"to_excel\"] = hazard_letter\n",
    "            else:\n",
    "                update_hazard_group_in_json(value, hazard_letter)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            update_hazard_group_in_json(item, hazard_letter)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rw5iAPauYNhB"
   },
   "outputs": [],
   "source": [
    "def fill_hazard_group_rag(\n",
    "    llm,\n",
    "    db,\n",
    "    json_input: Dict[str, Any],\n",
    "    source_match: str,\n",
    "    content: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts hazard H-codes from a document and assigns a hazard letter (A..N) to all 'hazard_group' fields in a JSON.\n",
    "    Steps:\n",
    "    1. Extract all H-codes (e.g., H300, H315) from the text `content`.\n",
    "    2. Map each code to a hazard letter using a global `hazard_classification` table.\n",
    "       - Fallback classification is provided if not defined globally.\n",
    "       - Unknown codes default to letter 'E'.\n",
    "    3. Determine the most severe hazard letter among all detected codes (priority A > B > C > D > E > N).\n",
    "    4. Recursively update the JSON:\n",
    "       - Each 'hazard_group' dictionary has its 'response' and 'to_excel' set to the selected letter.\n",
    "    Args:\n",
    "        llm: Language model object (not used directly here, kept for compatibility).\n",
    "        db: Database object (not used directly here, kept for compatibility).\n",
    "        json_input (dict): Nested JSON containing hazard fields to update.\n",
    "        source_match (str): Source document identifier (used in logging).\n",
    "        content (str): Text content of the document from which H-codes are extracted.\n",
    "    Returns:\n",
    "        dict: The updated JSON with all 'hazard_group' fields filled with the appropriate hazard letter.\n",
    "    Notes:\n",
    "        - If no H-codes are detected, assigns 'N' (lowest hazard).\n",
    "        - Prints debug information including detected codes, mapped letters, and final letter.\n",
    "    \"\"\"\n",
    "\n",
    "    # Global classification table (fallback if not defined)\n",
    "    global hazard_classification\n",
    "    if \"hazard_classification\" not in globals():\n",
    "        hazard_classification = {\n",
    "            \"N\": [],\n",
    "            \"E\": [\"H303\", \"H305\", \"H313\", \"H316\", \"H318\", \"H320\", \"H333\"],\n",
    "            \"D\": [\"H302\", \"H312\", \"H332\", \"H315\", \"H319\"],\n",
    "            \"C\": [\"H341\", \"H351\", \"H361\", \"H362\", \"H371\", \"H373\", \"H317\", \"H335\", \"H336\"],\n",
    "            \"B\": [\"H301\", \"H304\", \"H311\", \"H331\", \"H334\", \"H314\", \"H318\"],\n",
    "            \"A\": [\"H300\", \"H310\", \"H330\", \"H340\", \"H350\", \"H360\", \"H370\", \"H372\"]\n",
    "        }\n",
    "\n",
    "    # Severity priority (A = most severe)\n",
    "    priority = [\"A\", \"B\", \"C\", \"D\", \"E\", \"N\"]\n",
    "\n",
    "    # Build code -> letter mapping\n",
    "    code_to_letter = {code: letter for letter in priority for code in hazard_classification.get(letter, [])}\n",
    "\n",
    "    # Extract all H-codes from the text (normalized)\n",
    "    h_codes = re.findall(r'H\\s*\\d{3}', (content or \"\").upper())\n",
    "    h_codes = [c.replace(\" \", \"\") for c in h_codes]\n",
    "\n",
    "    if not h_codes:\n",
    "        hazard_letter = \"N\"\n",
    "        print(f\"No H-codes detected in {source_match}\")\n",
    "    else:\n",
    "        # Map to letters, unknown codes = \"E\"\n",
    "        detected_letters = {code_to_letter.get(c, \"E\") for c in h_codes}\n",
    "\n",
    "        # Select the most severe\n",
    "        hazard_letter = next((letter for letter in priority if letter in detected_letters), \"N\")\n",
    "\n",
    "        print(f\"H-codes detected in {source_match}: {h_codes}\")\n",
    "        print(f\"Letters detected: {detected_letters}\")\n",
    "        print(f\"Final hazard letter: {hazard_letter}\")\n",
    "\n",
    "    # Update JSON recursively\n",
    "    def _update_hazard_group(node):\n",
    "        if isinstance(node, dict):\n",
    "            for key, value in node.items():\n",
    "                if key == \"hazard_group\" and isinstance(value, dict):\n",
    "                    value[\"response\"] = hazard_letter\n",
    "                    value[\"to_excel\"] = hazard_letter\n",
    "                else:\n",
    "                    _update_hazard_group(value)\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                _update_hazard_group(item)\n",
    "\n",
    "    _update_hazard_group(json_input)\n",
    "    return json_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kas7Z2LKMeIs"
   },
   "outputs": [],
   "source": [
    "# Call the function: updated_json_Hazards\n",
    "\n",
    "updated_json_Hazards = fill_hazard_group_rag(\n",
    "    llm=llm,\n",
    "    db=db,\n",
    "    json_input=json_hazards,\n",
    "    source_match=source_match,\n",
    "    content=content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UQnBQQwKI5f"
   },
   "outputs": [],
   "source": [
    "# Call the function: updated_json_Waste_disposal_measures\n",
    "\n",
    "updated_json_Waste_disposal_measures = fill_hazard_group_rag(\n",
    "    llm=llm,\n",
    "    db=db,\n",
    "    json_input=json_waste_disposal_measures,\n",
    "    source_match=source_match,\n",
    "    content=content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PsRVrNHKJK3"
   },
   "outputs": [],
   "source": [
    "# Call the function: updated_json_Storage\n",
    "\n",
    "updated_json_Storage = fill_hazard_group_rag(\n",
    "    llm=llm,\n",
    "    db=db,\n",
    "    json_input=json_storage,\n",
    "    source_match=source_match,\n",
    "    content=content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTuBDx-NbkTx"
   },
   "source": [
    "# Severity and Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MZCFpuqtYag"
   },
   "outputs": [],
   "source": [
    "def fill_json_severity_probability(json_input: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Populates the severity and probability fields in a nested JSON for risk assessment.\n",
    "    Specifically updates the following fields if present in 'Sheet_2':\n",
    "    - 'severity'\n",
    "    - 'likelihood_before_control_measures'\n",
    "    - 'likelihood_after_control_measures'\n",
    "    Args:\n",
    "        json_input (dict): Nested JSON expected to contain a 'Sheet_2' key with relevant fields.\n",
    "    Returns:\n",
    "        dict: The same JSON with updated 'response' and 'to_excel' values for the specified fields.\n",
    "    Notes:\n",
    "        - Default values applied:\n",
    "            • severity -> \"Severe\"\n",
    "            • likelihood_before_control_measures -> \"Possible\"\n",
    "            • likelihood_after_control_measures -> \"Unlikely\"\n",
    "        - Other fields in the JSON remain untouched.\n",
    "    \"\"\"\n",
    "\n",
    "    sheet_key = \"Sheet_2\"\n",
    "    sheet = json_input.get(sheet_key)\n",
    "    if not isinstance(sheet, dict):\n",
    "        raise ValueError(f\"'{sheet_key}' not found in the JSON\")\n",
    "\n",
    "    # Default values according to business logic\n",
    "    if isinstance(sheet.get(\"severity\"), dict):\n",
    "        sheet[\"severity\"][\"response\"] = \"Severe\"\n",
    "        sheet[\"severity\"][\"to_excel\"] = \"Severe\"\n",
    "\n",
    "    if isinstance(sheet.get(\"likelihood_before_control_measures\"), dict):\n",
    "        sheet[\"likelihood_before_control_measures\"][\"response\"] = \"Possible\"\n",
    "        sheet[\"likelihood_before_control_measures\"][\"to_excel\"] = \"Possible\"\n",
    "\n",
    "    if isinstance(sheet.get(\"likelihood_after_control_measures\"), dict):\n",
    "        sheet[\"likelihood_after_control_measures\"][\"response\"] = \"Unlikely\"\n",
    "        sheet[\"likelihood_after_control_measures\"][\"to_excel\"] = \"Unlikely\"\n",
    "\n",
    "    return json_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqpNklTPtbUD"
   },
   "outputs": [],
   "source": [
    "# Fill severity and probability fields in the JSON\n",
    "result = fill_json_severity_probability(json_hazards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTyO0HF1csb_"
   },
   "source": [
    "# Hazards Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XAJ7J3FcvOo"
   },
   "outputs": [],
   "source": [
    "# Required fields for data type\n",
    "\n",
    "hazards_fields_dtr = [\n",
    "    \"physical_form_and_quantity\",\n",
    "    \"potential_routes_of_exposure\",\n",
    "    \"workplace_exposure_limits\",\n",
    "    \"arising_harm\"\n",
    "]\n",
    "\n",
    "waste_disposal_measures_fields_dtr = [\n",
    "    \"handling_of_the_product_if_it_becomes_waste\"\n",
    "]\n",
    "\n",
    "spill_management_fields_dtr = [\n",
    "    \"details\"\n",
    "]\n",
    "\n",
    "fire_procedures_fields_dtr = [\n",
    "    \"details\"\n",
    "]\n",
    "\n",
    "first_aid_procedures_fields_dtr = [\n",
    "    \"eyes\",\n",
    "    \"skin\",\n",
    "    \"if_ingested\",\n",
    "    \"if_inhaled\"\n",
    "]\n",
    "\n",
    "storage_fields_dtr = [\n",
    "    \"hazard_label_and_store_safely_on_shelf\",\n",
    "    \"special_storage_describe\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fToLwrHgenlb"
   },
   "outputs": [],
   "source": [
    "dtr_tables = [\n",
    "    \"Hazards\",\n",
    "    \"Waste disposal measures, disposal waste\",\n",
    "    \"Spill management, spills, information_and_details_about_Spill_management\",\n",
    "    \"Fire procedures, Fire Fighting Measures, information_and_details_about_Fire_procedures\",\n",
    "    \"First aid procedures, First Aid Measures\",\n",
    "    \"Storage, Safe Storage\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLEUEW7BZcMf"
   },
   "outputs": [],
   "source": [
    "def extract_hazards_text(\n",
    "    source_match: str,\n",
    "    json_input: Dict[str, Any],\n",
    "    llm,\n",
    "    content: str,\n",
    "    fields_list: Optional[List[str]] = None,\n",
    "    table_index: int = 0\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts hazard-related information from an SDS/MSDS document and populates a JSON.\n",
    "    This function focuses on extracting the following hazard fields:\n",
    "    - physical_form_and_quantity\n",
    "    - potential_routes_of_exposure\n",
    "    - workplace_exposure_limits\n",
    "    - arising_harm\n",
    "    It uses hierarchical prompting with an LLM to first select the relevant context\n",
    "    and then generate a full answer, including a concise Excel-ready summary.\n",
    "    Args:\n",
    "        source_match (str): Filename or document reference for tracking/logging.\n",
    "        json_input (dict): JSON structure to populate (must contain 'Sheet_2').\n",
    "        llm: LLM object with a .predict() method to query.\n",
    "        content (str): Full SDS/MSDS document text.\n",
    "        fields_list (List[str], optional): Specific fields to extract. Defaults to all hazard fields.\n",
    "        table_index (int, optional): Index of the table (reserved for future use).\n",
    "    Returns:\n",
    "        dict: Updated JSON with 'response' (full text) and 'to_excel' (concise summary) for each field.\n",
    "    Notes:\n",
    "        - If no relevant information is found, 'to_excel' will be \"N/A\".\n",
    "        - EXCEL_SUMMARY is generated in English, max 50 words / 200 characters.\n",
    "        - The LLM is strictly instructed not to invent any information.\n",
    "    \"\"\"\n",
    "\n",
    "    excel_na_to_excel = \"N/A\"\n",
    "    sheet_key = \"Sheet_2\"\n",
    "    max_excel_chars = 300\n",
    "\n",
    "    if sheet_key not in json_input:\n",
    "        raise ValueError(f\"Sheet key '{sheet_key}' not found in json_input\")\n",
    "\n",
    "    # Questions for each field\n",
    "    questions = {\n",
    "        \"physical_form_and_quantity\": (\n",
    "            \"What is the physical form of the substance (gas, liquid, solid) \"\n",
    "            \"and in what packaging or quantity format is it supplied (e.g., bottle 200 ml, bag, sack, cylinder)?\"\n",
    "        ),\n",
    "        \"potential_routes_of_exposure\": (\n",
    "            \"What are the possible routes of exposure to the substance for humans? \"\n",
    "            \"(e.g., inhalation, skin contact, eye contact, ingestion).\"\n",
    "        ),\n",
    "        \"workplace_exposure_limits\": (\n",
    "            \"What are the Workplace Exposure Limits (WEL), TWA (8h), STEL (15 min), or other exposure thresholds \"\n",
    "            \"provided? Include numeric values and units.\"\n",
    "        ),\n",
    "        \"arising_harm\": (\n",
    "            \"What are the potential harms or adverse effects associated with exposure to this substance? \"\n",
    "            \"(e.g., toxic effects, respiratory issues, organ damage, skin/eye irritation).\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Context selector prompt\n",
    "    prompt_selector = (\n",
    "        \"You are an intelligent assistant specialized in analyzing Safety Data Sheets (SDS).\\n\"\n",
    "        \"You will be given the full SDS text and a SECTION name.\\n\"\n",
    "        \"Task: Extract only the sentences and fragments relevant to the SECTION. \"\n",
    "        \"If nothing is relevant, return an empty string.\\n\\n\"\n",
    "        \"SECTION: {section}\\n\\n\"\n",
    "        \"DOCUMENT:\\n{fragments}\\n\\n\"\n",
    "        \"Return ONLY the relevant CONTEXT text.\"\n",
    "    )\n",
    "\n",
    "    # Final response prompt template\n",
    "    prompt_template = (\n",
    "        \"You are a precise assistant specialized in Safety Data Sheets.\\n\"\n",
    "        \"Answer STRICTLY using only the provided CONTEXT. Do not invent or add external info.\\n\\n\"\n",
    "        \"If the document contains no relevant information, answer: 'The document does not provide this information.'\\n\\n\"\n",
    "        \"At the end of your answer, ALWAYS add a final line:\\n\"\n",
    "        \"EXCEL_SUMMARY: <short summary or 'no information'>\\n\\n\"\n",
    "        \"Rules for EXCEL_SUMMARY:\\n\"\n",
    "        \" - If no info: EXCEL_SUMMARY: no information\\n\"\n",
    "        \" - If info exists: concise summary (max 50 words, max 200 chars), in English\\n\"\n",
    "        \" - Prefer keywords, numbers, hazard codes, short phrases, comma-separated\\n\\n\"\n",
    "        \"QUESTION: {question}\\n\\n\"\n",
    "        \"CONTEXT:\\n{context}\"\n",
    "    )\n",
    "\n",
    "    excel_marker_re = re.compile(r\"EXCEL_SUMMARY:\\s*(.+)$\", re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "    # Process each field\n",
    "    for field in fields_list:\n",
    "        cell = json_input[sheet_key].get(field, {})\n",
    "        question = questions.get(field, f\"Extract the information about {field}.\")\n",
    "\n",
    "        # Step 1: Context selector\n",
    "        selector_prompt = prompt_selector.format(\n",
    "            section=field,\n",
    "            fragments=content\n",
    "        )\n",
    "        try:\n",
    "            context_filtered = llm.predict(selector_prompt).strip()\n",
    "        except Exception as e:\n",
    "            context_filtered = \"\"\n",
    "            print(f\"Error in context selector for '{field}': {e}\")\n",
    "\n",
    "        # Step 2: Final answer\n",
    "        final_prompt = prompt_template.format(\n",
    "            question=question,\n",
    "            context=context_filtered\n",
    "        )\n",
    "        try:\n",
    "            full_response = llm.predict(final_prompt).strip()\n",
    "        except Exception as e:\n",
    "            full_response = \"\"\n",
    "            print(f\"Error in final response for '{field}': {e}\")\n",
    "\n",
    "        # Extract EXCEL_SUMMARY\n",
    "        m = excel_marker_re.search(full_response)\n",
    "        if m:\n",
    "            excel_summary = m.group(1).strip()\n",
    "            to_excel_value = (\n",
    "                excel_na_to_excel\n",
    "                if excel_summary.lower() in (\"no information\", \"not available\", \"no information available\")\n",
    "                else excel_summary[:max_excel_chars].rstrip()\n",
    "            )\n",
    "        else:\n",
    "            to_excel_value = excel_na_to_excel\n",
    "\n",
    "        # Update JSON\n",
    "        json_input[sheet_key][field][\"response\"] = full_response\n",
    "        json_input[sheet_key][field][\"to_excel\"] = to_excel_value\n",
    "\n",
    "        print(\"-----\"*80)\n",
    "        print(f\"Field: '{field}'\")\n",
    "        print(f\"response: {full_response}\")\n",
    "        print(f\"to_excel: {to_excel_value}\")\n",
    "        print(f\"Document used for '{field}': {source_match}\")\n",
    "\n",
    "    return json_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcSEDEE3e1A3"
   },
   "outputs": [],
   "source": [
    "# Call the function: updated_json_Hazards\n",
    "updated_json_Hazards = extract_hazards_text(\n",
    "    source_match=source_match,\n",
    "    json_input=updated_json_Hazards,\n",
    "    llm=llm,\n",
    "    content=content,\n",
    "    fields_list= hazards_fields_dtr,\n",
    "    table_index=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4HaLUavWnAI"
   },
   "outputs": [],
   "source": [
    "def general_text_extraction(\n",
    "    source_match: str,\n",
    "    json_input: Dict[str, Any],\n",
    "    llm,\n",
    "    content: str,\n",
    "    fields_list: Optional[List[str]] = None,\n",
    "    table_index: int = 0\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Performs hierarchical extraction of information from a full SDS/MSDS document.\n",
    "    This function iterates over a list of fields in the JSON, extracts only the relevant context\n",
    "    from the document for each field using a two-step LLM process (context selection + detailed answer),\n",
    "    and produces an Excel-ready summary (EXCEL_SUMMARY) for each field.\n",
    "    Args:\n",
    "        source_match (str): Document filename or identifier for tracking/logging.\n",
    "        json_input (Dict[str, Any]): JSON structure to populate (must contain 'Sheet_2').\n",
    "        llm: LLM object with a .predict() method.\n",
    "        content (str): Full text content of the SDS/MSDS document.\n",
    "        fields_list (List[str], optional): List of field keys to extract. Defaults to all fields in 'Sheet_2'.\n",
    "        table_index (int, optional): Index of the table or section for context (default is 0).\n",
    "    Returns:\n",
    "        Dict[str, Any]: Updated JSON with the following for each field:\n",
    "            - 'response': full LLM answer for the field.\n",
    "            - 'to_excel': concise, Excel-friendly summary.\n",
    "    Notes:\n",
    "        - EXCEL_SUMMARY is generated in English, max 50 words / 200 characters.\n",
    "        - If no information is found for a field, 'to_excel' will be \"N/A\".\n",
    "        - The function ensures the original JSON structure is preserved.\n",
    "        - Strictly instructs the LLM not to invent information beyond the document content.\n",
    "    \"\"\"\n",
    "\n",
    "    excel_na_to_excel = \"N/A\"\n",
    "    sheet_key = \"Sheet_2\"\n",
    "    max_excel_chars = 300\n",
    "\n",
    "    if fields_list is None:\n",
    "        fields_list = list(json_input[sheet_key].keys())\n",
    "\n",
    "    # Use full document content\n",
    "    full_document_text = content.strip()\n",
    "\n",
    "    # Prompts\n",
    "    prompt_selector = (\n",
    "        \"You are an intelligent assistant specialized in analyzing Safety Data Sheets (SDS).\\n\"\n",
    "        \"You will be given a full SDS document and a target SECTION name.\\n\"\n",
    "        \"Task: Read the document, understand the whole context, and produce a single coherent CONTEXT text\\n\"\n",
    "        \"that contains only the information relevant to the SECTION. If there is no relevant information, return an empty string.\\n\\n\"\n",
    "        \"SECTION: {section}\\n\\n\"\n",
    "        \"DOCUMENT:\\n{fragments}\\n\\n\"\n",
    "        \"Return ONLY the CONTEXT text (no JSON, no explanation).\"\n",
    "    )\n",
    "\n",
    "    prompt_template = (\n",
    "        \"You are a precise technical assistant specialized in Safety Data Sheets. \"\n",
    "        \"Answer STRICTLY using only the content retrieved from the provided document. \"\n",
    "        \"Do not invent or add external information. If the document contains no information \"\n",
    "        \"relevant to the question, state explicitly that the information is not available.\\n\\n\"\n",
    "        \"If the QUESTION requests only 'details' without specifying more, provide a comprehensive summary of ALL information \"\n",
    "        \"received from the document or retrieved context.\\n\\n\"\n",
    "        \"At the end of your answer, ALWAYS add a final line that starts exactly with:\\n\"\n",
    "        \"EXCEL_SUMMARY: <one-line summary or 'no information'>\\n\\n\"\n",
    "        \"Rules for the EXCEL_SUMMARY (VERY IMPORTANT):\\n\"\n",
    "        \" - If there is no relevant information, write exactly: EXCEL_SUMMARY: no information\\n\"\n",
    "        \" - If there IS relevant information, use a concise summary suitable for a single Excel cell:\\n\"\n",
    "        \"   • Keep it very short: max 50 words and max 200 characters.\\n\"\n",
    "        \"   • Prefer keywords, numeric values, hazard codes (e.g., H315), or short phrases.\\n\"\n",
    "        \"   • If multiple small items, use comma-separated short phrases (no newlines).\\n\"\n",
    "        \" - The EXCEL_SUMMARY must always be in English.\\n\\n\"\n",
    "        \"Now answer the QUESTION using only document content.\"\n",
    "    )\n",
    "\n",
    "    excel_marker_re = re.compile(r\"EXCEL_SUMMARY:\\s*(.+)$\", re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "    # Section name\n",
    "    section_name = dtr_tables[table_index]\n",
    "\n",
    "    # Process each field\n",
    "    for campo in fields_list:\n",
    "        cell = json_input[sheet_key].get(campo, {})\n",
    "        consulta = str(cell.get(\"content\", \"\") or \"\").strip()\n",
    "\n",
    "        if not consulta:\n",
    "            json_input[sheet_key][campo] = {\n",
    "                \"content\": cell.get(\"content\", \"\"),\n",
    "                \"position\": cell.get(\"position\", \"\"),\n",
    "                \"response\": \"\",\n",
    "                \"to_excel\": excel_na_to_excel\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        # Step 1: Context selection\n",
    "        selector_prompt = prompt_selector.format(\n",
    "            section=section_name,\n",
    "            fragments=full_document_text\n",
    "        )\n",
    "        try:\n",
    "            context_filtered = llm.predict(selector_prompt).strip()\n",
    "        except Exception as e:\n",
    "            context_filtered = \"\"\n",
    "            print(f\"Error in context selector for field '{campo}': {e}\")\n",
    "\n",
    "        # Step 2: Final response\n",
    "        final_prompt = (\n",
    "            f\"{prompt_template}\\n\\nQUESTION: {consulta}\\n\\nCONTEXT:\\n{context_filtered}\"\n",
    "        )\n",
    "        try:\n",
    "            respuesta_completa = llm.predict(final_prompt).strip()\n",
    "        except Exception as e:\n",
    "            respuesta_completa = \"\"\n",
    "            print(f\"Error in final response for field '{campo}': {e}\")\n",
    "\n",
    "        # Extract EXCEL_SUMMARY\n",
    "        m = excel_marker_re.search(respuesta_completa)\n",
    "        if m:\n",
    "            excel_summary = m.group(1).strip()\n",
    "            to_excel_value = (\n",
    "                excel_na_to_excel\n",
    "                if excel_summary.lower() in (\"no information\", \"no information available\", \"not available\")\n",
    "                else excel_summary[:max_excel_chars].rstrip()\n",
    "            )\n",
    "        else:\n",
    "            to_excel_value = excel_na_to_excel\n",
    "\n",
    "        # Save in JSON\n",
    "        json_input[sheet_key][campo][\"response\"] = respuesta_completa\n",
    "        json_input[sheet_key][campo][\"to_excel\"] = to_excel_value\n",
    "\n",
    "        # Source\n",
    "        print(\"-----\"*80)\n",
    "        print(f\"Field: '{campo}'\")\n",
    "        print(f\"response: {respuesta_completa}\")\n",
    "        print(f\"to_excel: {to_excel_value}\")\n",
    "        print(f\"Document used for '{campo}': {source_match}\")\n",
    "\n",
    "    return json_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ud5MPXm6e1fQ"
   },
   "outputs": [],
   "source": [
    "# Call the function: updated_json_Waste_disposal_measures\n",
    "\n",
    "updated_json_Waste_disposal_measures = general_text_extraction(\n",
    "    source_match=source_match,\n",
    "    json_input=updated_json_Waste_disposal_measures,\n",
    "    llm=llm,\n",
    "    content = content,\n",
    "    fields_list= waste_disposal_measures_fields_dtr,\n",
    "    table_index=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0_yi5ope4NI"
   },
   "outputs": [],
   "source": [
    "# Call the function: updated_json_Spill_management\n",
    "\n",
    "updated_json_Spill_management = general_text_extraction(\n",
    "    source_match=source_match,\n",
    "    json_input=updated_json_spill_management,\n",
    "    content = content,\n",
    "    llm=llm,\n",
    "    fields_list= spill_management_fields_dtr,\n",
    "    table_index=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JkZIJL7e5ay"
   },
   "outputs": [],
   "source": [
    "# Call the function: updated_json_Fire_procedures\n",
    "\n",
    "updated_json_Fire_procedures = general_text_extraction(\n",
    "    source_match=source_match,\n",
    "    json_input=updated_json_fire_procedures,\n",
    "    content = content,\n",
    "    llm=llm,\n",
    "    fields_list= fire_procedures_fields_dtr ,\n",
    "    table_index=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5gHDuV7e7Gq"
   },
   "outputs": [],
   "source": [
    "# Call the function: updated_json_First_aid_procedures\n",
    "updated_json_First_aid_procedures = general_text_extraction(\n",
    "    source_match=source_match,\n",
    "    json_input=updated_json_first_aid_procedures,\n",
    "    content = content,\n",
    "    llm=llm,\n",
    "    fields_list= first_aid_procedures_fields_dtr,\n",
    "    table_index=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnnBHk7Fe8p0"
   },
   "outputs": [],
   "source": [
    "# Call the function: updated_json_Storage\n",
    "\n",
    "updated_json_Storage = general_text_extraction(\n",
    "    source_match=source_match,\n",
    "    json_input=updated_json_storage,\n",
    "    content = content,\n",
    "    llm=llm,\n",
    "    fields_list= storage_fields_dtr,\n",
    "    table_index=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MTUJ7XEITSi"
   },
   "outputs": [],
   "source": [
    "# Pretty-print the updated JSON\n",
    "print(json.dumps(json_first_aid_procedures, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPSXwiVge-ha"
   },
   "source": [
    "# Fill Excel and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CCeWGbcV5c5"
   },
   "outputs": [],
   "source": [
    "# Call the function: list_of_jsons_to_excel\n",
    "list_of_jsons_to_excel = [\n",
    "    updated_json_Waste_disposal_measures,\n",
    "    updated_json_Storage,\n",
    "    updated_json_Fire_procedures,\n",
    "    updated_json_First_aid_procedures,\n",
    "    updated_json_Hazards,\n",
    "    updated_json_Spill_management\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_NE_CLyGYtO"
   },
   "outputs": [],
   "source": [
    "def fill_excel_with_json(jsons_list: list, template_path: str, output_dir: str, source_match: str):\n",
    "    wb = openpyxl.load_workbook(template_path)\n",
    "    ws = wb['COSHH Assessment']\n",
    "\n",
    "    standard_font = Font(name=\"Arial\", size=12, bold=True)\n",
    "    standard_alignment = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "\n",
    "    multi_position_fields = {\n",
    "        \"likelihood_before_control_measures\",\n",
    "        \"severity\",\n",
    "        \"likelihood_after_control_measures\"\n",
    "    }\n",
    "\n",
    "    for json_data in jsons_list:\n",
    "        for field, values in json_data.get(\"Sheet_2\", {}).items():\n",
    "            position = values.get(\"position\")\n",
    "            to_excel = values.get(\"to_excel\")\n",
    "\n",
    "            if not position or not to_excel:\n",
    "                continue\n",
    "\n",
    "            if field in multi_position_fields and isinstance(position, list):\n",
    "                for pos in position:\n",
    "                    cell = ws[pos]\n",
    "                    cell.value = to_excel\n",
    "                    cell.font = standard_font\n",
    "                    cell.alignment = standard_alignment\n",
    "            elif isinstance(position, str):\n",
    "                cell = ws[position]\n",
    "                cell.value = to_excel\n",
    "                cell.font = standard_font\n",
    "                cell.alignment = standard_alignment\n",
    "\n",
    "    datetime_str = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "    filename = f\"{source_match}_{datetime_str}.xlsx\"\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    wb.save(output_path)\n",
    "    wb.close()\n",
    "\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZcTy_x1ONjS"
   },
   "outputs": [],
   "source": [
    "# Call the function\n",
    "excel_created = fill_excel_with_json(\n",
    "    list_of_jsons_to_excel,\n",
    "    template_path,\n",
    "    output_Excel,\n",
    "    source_match=source_match\n",
    ")\n",
    "\n",
    "print(\"Excel generated at:\", excel_created)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "82TZ6HYTeHX2",
    "OTuBDx-NbkTx"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
